"""
DebateAI Backend - FastAPI æ‡‰ç”¨

Phase 3c: LangGraph ToolNode æ¶æ§‹
- Agent ç¯€é»åªè² è²¬æ±ºç­–ï¼ŒToolNode è² è²¬åŸ·è¡Œå·¥å…·
- astream_events(version="v2") å¯æ­£ç¢ºæ•ç² on_tool_start/on_tool_end
- ä¿®å¾©æœå°‹æŒ‡ç¤ºå™¨ç„¡æ³•é¡¯ç¤ºçš„å•é¡Œ
"""

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from dotenv import load_dotenv
import asyncio
import json
import re
import os
import logging

# Phase 3c: é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s'
)
logger = logging.getLogger(__name__)

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

app = FastAPI(title="DebateAI API", version="0.4.0")


# ============================================================
# ç’°å¢ƒè®Šæ•¸
# ============================================================
USE_FAKE_STREAM = os.getenv("USE_FAKE_STREAM", "false").lower() == "true"
USE_LANGGRAPH = os.getenv("USE_LANGGRAPH", "true").lower() == "true"  # Phase 3b: é è¨­ä½¿ç”¨ LangGraph + astream_events
GROQ_API_KEY = os.getenv("GROQ_API_KEY", "")
HAS_GROQ_KEY = bool(GROQ_API_KEY and len(GROQ_API_KEY) > 10)
GROQ_MODEL = os.getenv("GROQ_MODEL", "llama-3.1-8b-instant")


# ============================================================
# Regex CORS Middleware
# ============================================================
class RegexCORSMiddleware(CORSMiddleware):
    """æ”¯æ´ regex åŒ¹é…çš„ CORS Middleware"""
    def is_allowed_origin(self, origin: str) -> bool:
        if not origin:
            return False
        if origin.startswith("http://localhost"):
            return True
        if re.match(r"https://.*\.pages\.dev$", origin):
            return True
        if re.match(r"https://.*\.ggff\.net$", origin):
            return True
        allowed = os.getenv("ALLOWED_ORIGINS", "")
        if allowed and allowed != "*":
            allowed_list = [o.strip() for o in allowed.split(",") if o.strip()]
            if origin in allowed_list:
                return True
        return super().is_allowed_origin(origin)

app.add_middleware(
    RegexCORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["Content-Type", "Authorization"],
    allow_credentials=True,
)


# ============================================================
# è«‹æ±‚æ¨¡å‹
# ============================================================
class DebateRequest(BaseModel):
    topic: str = Field(..., min_length=1, max_length=200, description="è¾¯è«–ä¸»é¡Œï¼Œæœ€å¤š 200 å­—")
    max_rounds: int = Field(default=3, ge=1, le=5, description="è¾¯è«–è¼ªæ•¸ï¼Œ1-5 è¼ª")
    language: str = Field(default="zh", pattern="^(zh|en)$", description="èªè¨€è¨­å®šï¼šzh (ç¹é«”ä¸­æ–‡) æˆ– en (English)")


# ============================================================
# SSE è¼”åŠ©å‡½æ•¸
# ============================================================
def sse_event(data: dict) -> str:
    """ç”Ÿæˆ SSE äº‹ä»¶æ ¼å¼"""
    return f"data: {json.dumps(data)}\n\n"


# ============================================================
# Fake SSE ä¸²æµï¼ˆFallbackï¼‰
# ============================================================
async def fake_debate_stream(topic: str, max_rounds: int = 3, language: str = "zh"):
    """Phase 1 æ¸¬è©¦ç”¨ï¼šæ¨¡æ“¬ AI è¾¯è«–"""
    is_en = language == "en"

    msg_init = 'âš¡ [FAKE MODE] Waking up simulation engine...' if is_en else 'âš¡ [FAKE MODE] æ­£åœ¨å–šé†’æ¨¡æ“¬å¼•æ“...'
    msg_ready = 'ğŸ”¥ Simulation engine ready!' if is_en else 'ğŸ”¥ æ¨¡æ“¬å¼•æ“å·²å°±ç·’ï¼'

    yield sse_event({'type': 'status', 'text': msg_init})
    await asyncio.sleep(0.3)

    yield sse_event({'type': 'status', 'text': msg_ready})
    
    for round_num in range(1, max_rounds + 1):
        # Optimist
        round_label = f'Round {round_num}' if is_en else f'ç¬¬ {round_num} è¼ª'
        yield sse_event({'type': 'speaker', 'node': 'optimist', 'text': round_label})

        if is_en:
            optimist_text = f"Regarding '{topic}', I believe this is full of opportunities! Technological progress always brings new possibilities."
        else:
            optimist_text = f"é—œæ–¼ã€Œ{topic}ã€ï¼Œæˆ‘èªç‚ºé€™æ˜¯å……æ»¿æ©Ÿæœƒçš„ï¼ç§‘æŠ€é€²æ­¥ç¸½æ˜¯å¸¶ä¾†æ–°çš„å¯èƒ½æ€§ã€‚"

        for char in optimist_text:
            yield sse_event({'type': 'token', 'node': 'optimist', 'text': char})
            await asyncio.sleep(0.02)

        yield sse_event({'type': 'speaker_end', 'node': 'optimist'})

        # Skeptic
        yield sse_event({'type': 'speaker', 'node': 'skeptic', 'text': round_label})

        if is_en:
            skeptic_text = f"However, we must be cautious about '{topic}'. Blind optimism may lead to overlooking risks."
        else:
            skeptic_text = f"ç„¶è€Œï¼Œæˆ‘å€‘å¿…é ˆè¬¹æ…çœ‹å¾…ã€Œ{topic}ã€ã€‚ç›²ç›®æ¨‚è§€å¯èƒ½å°è‡´å¿½è¦–é¢¨éšªã€‚"

        for char in skeptic_text:
            yield sse_event({'type': 'token', 'node': 'skeptic', 'text': char})
            await asyncio.sleep(0.02)

        yield sse_event({'type': 'speaker_end', 'node': 'skeptic'})

    msg_complete = f'âœ… [FAKE] Debate ended! {max_rounds} rounds.' if is_en else f'âœ… [FAKE] è¾¯è«–çµæŸï¼å…± {max_rounds} è¼ªã€‚'
    yield sse_event({'type': 'complete', 'text': msg_complete})


# ============================================================
# çœŸå¯¦ LLM ä¸²æµ
# ============================================================
async def real_debate_stream(topic: str, max_rounds: int = 3, language: str = "zh"):
    """Phase 2: çœŸæ­£çš„ Token-Level ä¸²æµ"""
    from app.graph import (
        get_llm,
        create_initial_state,
        build_prompt,
        update_state_after_speaker
    )

    is_en = language == "en"

    # i18n è¨Šæ¯
    msg_init = 'âš¡ Connecting to AI Debate Engine...' if is_en else 'âš¡ æ­£åœ¨å–šé†’ AI è¾¯è«–å¼•æ“...'
    msg_model = f'ğŸ”¥ Using model: {GROQ_MODEL}' if is_en else f'ğŸ”¥ ä½¿ç”¨æ¨¡å‹: {GROQ_MODEL}'
    msg_llm_init_fail = 'LLM initialization failed: ' if is_en else 'LLM åˆå§‹åŒ–å¤±æ•—: '
    msg_llm_stream_fail = 'LLM stream interrupted: ' if is_en else 'LLM ä¸²æµä¸­æ–·: '
    msg_debate_error = 'âŒ Debate stopped due to error' if is_en else 'âŒ è¾¯è«–å› éŒ¯èª¤è€Œä¸­æ–·'
    msg_empty_response = 'LLM returned empty response' if is_en else 'LLM è¿”å›ç©ºå›æ‡‰'

    yield sse_event({'type': 'status', 'text': msg_init})

    # åˆå§‹åŒ–ï¼ˆlanguage å·²æ•´åˆé€² stateï¼‰
    state = create_initial_state(topic, max_rounds, language)

    try:
        llm = get_llm()
        yield sse_event({'type': 'status', 'text': msg_model})
    except Exception as e:
        yield sse_event({'type': 'error', 'text': f'{msg_llm_init_fail}{str(e)}'})
        return

    # è¾¯è«–å¾ªç’°
    while state['current_speaker'] != 'end':
        speaker = state['current_speaker']
        round_num = state['round_count'] + 1

        # ç™¼é€ speaker é–‹å§‹äº‹ä»¶
        round_label = f'Round {round_num}' if is_en else f'ç¬¬ {round_num} è¼ª'
        yield sse_event({'type': 'speaker', 'node': speaker, 'text': round_label})

        # å»ºæ§‹ prompt
        messages = build_prompt(state, speaker)

        # ç›´æ¥å‘¼å« llm.astream() å¯¦ç¾ token ä¸²æµ
        full_content = ""
        try:
            async for chunk in llm.astream(messages):
                if chunk.content:
                    full_content += chunk.content
                    yield sse_event({'type': 'token', 'node': speaker, 'text': chunk.content})
        except Exception as e:
            yield sse_event({'type': 'error', 'text': f'{msg_llm_stream_fail}{str(e)}'})
            yield sse_event({'type': 'speaker_end', 'node': speaker})
            yield sse_event({'type': 'complete', 'text': msg_debate_error})
            return

        # ç™¼é€ speaker çµæŸäº‹ä»¶
        yield sse_event({'type': 'speaker_end', 'node': speaker})

        # æ›´æ–°ç‹€æ…‹
        if full_content:
            state = update_state_after_speaker(state, speaker, full_content)
        else:
            yield sse_event({'type': 'error', 'text': msg_empty_response})
            break

    rounds_completed = state['round_count']
    msg_complete = f'âœ… Debate complete! {rounds_completed} exciting rounds.' if is_en else f'âœ… è¾¯è«–å®Œæˆï¼å…±é€²è¡Œäº† {rounds_completed} è¼ªç²¾å½©äº¤é‹’ã€‚'
    yield sse_event({'type': 'complete', 'text': msg_complete})


# ============================================================
# LangGraph StateGraph ä¸²æµï¼ˆPhase 3c - ToolNode æ¶æ§‹ï¼‰
# ============================================================
async def langgraph_debate_stream(topic: str, max_rounds: int = 3, language: str = "zh"):
    """Phase 3c: ä½¿ç”¨ ToolNode å¯¦ç¾å·¥å…·äº‹ä»¶è¿½è¹¤
    
    æ¶æ§‹æ”¹é€²ï¼š
    - Agent ç¯€é»åªè² è²¬æ±ºç­–ï¼ˆè¿”å› AIMessageï¼Œå¯èƒ½åŒ…å« tool_callsï¼‰
    - ToolNode ç¨ç«‹åŸ·è¡Œå·¥å…·ï¼ŒLangGraph è‡ªå‹•è§¸ç™¼ on_tool_start/on_tool_end
    - ä¿®å¾©æœå°‹æŒ‡ç¤ºå™¨ç„¡æ³•é¡¯ç¤ºçš„å•é¡Œ
    """
    from app.graph import debate_graph, create_initial_state

    logger.info(f"ğŸŒ langgraph_debate_stream received language: {language}")
    is_en = language == "en"

    yield sse_event({'type': 'status', 'text': 'âš¡ ' + ('Connecting to AI Debate Engine...' if is_en else 'æ­£åœ¨å–šé†’ AI è¾¯è«–å¼•æ“...')})
    yield sse_event({'type': 'status', 'text': f'ğŸ”¥ ' + ('Using model: ' if is_en else 'ä½¿ç”¨æ¨¡å‹: ') + f'{GROQ_MODEL} (LangGraph + Tools)'})

    # åˆå§‹åŒ–ï¼ˆlanguage å·²æ•´åˆé€² stateï¼‰
    state = create_initial_state(topic, max_rounds, language)
    
    current_node = None
    round_count = 0
    current_tool_query = None
    
    try:
        async for event in debate_graph.astream_events(
            state,
            version="v2"
        ):
            event_type = event.get("event")
            event_name = event.get("name", "")
            event_tags = event.get("tags", [])
            
            # Phase 3c: è¨ºæ–·æ—¥èªŒ
            logger.debug(f"Event: type={event_type}, name={event_name}, tags={event_tags}")
            
            # ç¯€é»é–‹å§‹
            if event_type == "on_chain_start":
                name = event.get("name", "")
                # Phase 3d: æ“´å±•æ”¯æ´ "moderator"
                if name in ("optimist", "skeptic", "moderator"):
                    if current_node and current_node != name:
                        yield sse_event({'type': 'speaker_end', 'node': current_node})
                        # âš ï¸ ç•¶ skeptic â†’ moderator æ™‚å¢åŠ è¼ªæ•¸ï¼ˆè¡¨ç¤ºä¸€è¼ªå®Œæˆï¼‰
                        if current_node == "skeptic" and name == "moderator":
                            round_count += 1
                    
                    current_node = name
                    
                    # æ ¹æ“šç¯€é»é¡å‹ç™¼é€ä¸åŒçš„ speaker äº‹ä»¶
                    if name == "moderator":
                        # Moderator é¡¯ç¤ºç‰¹æ®Šæ¨™è¨˜
                        moderator_text = 'Summary Report' if is_en else 'ç¸½çµå ±å‘Š'
                        yield sse_event({
                            'type': 'speaker',
                            'node': 'moderator',
                            'text': moderator_text
                        })
                    else:
                        # Optimist/Skeptic é¡¯ç¤ºè¼ªæ•¸
                        display_round = round_count + 1
                        round_text = f'Round {display_round}' if is_en else f'ç¬¬ {display_round} è¼ª'
                        yield sse_event({
                            'type': 'speaker',
                            'node': name,
                            'text': round_text
                        })

            # å·¥å…·é–‹å§‹
            elif event_type == "on_tool_start":
                tool_input = event.get("data", {}).get("input", {})
                unknown_query = "Unknown query" if is_en else "æœªçŸ¥æŸ¥è©¢"
                query = tool_input.get("query", unknown_query) if isinstance(tool_input, dict) else str(tool_input)
                current_tool_query = query
                yield sse_event({
                    'type': 'tool_start',
                    'tool': 'web_search',
                    'query': query,
                    'node': current_node or "unknown"
                })
            
            # å·¥å…·çµæŸï¼ˆæ­£å¸¸æˆ–éŒ¯èª¤ï¼‰
            elif event_type in ("on_tool_end", "on_tool_error"):
                yield sse_event({
                    'type': 'tool_end',
                    'tool': 'web_search',
                    'node': current_node or "unknown"
                })
                current_tool_query = None
            
            # LLM Token ä¸²æµ
            elif event_type == "on_chat_model_stream":
                chunk = event.get("data", {}).get("chunk")
                if chunk and hasattr(chunk, "content") and chunk.content:
                    # Phase 3d: æ“´å±•æ”¯æ´ moderator
                    if current_node in ("optimist", "skeptic", "moderator"):
                        yield sse_event({
                            'type': 'token',
                            'node': current_node,
                            'text': chunk.content
                        })
        
        # çµæŸ
        if current_node:
            yield sse_event({'type': 'speaker_end', 'node': current_node})

        msg_complete = f'âœ… Debate complete! {round_count} exciting rounds.' if is_en else f'âœ… è¾¯è«–å®Œæˆï¼å…±é€²è¡Œäº† {round_count} è¼ªç²¾å½©äº¤é‹’ã€‚'
        yield sse_event({
            'type': 'complete',
            'text': msg_complete
        })
    
    except Exception as e:
        # ç¢ºä¿å·¥å…·æŒ‡ç¤ºå™¨è¢«æ¸…é™¤
        if current_tool_query:
            yield sse_event({'type': 'tool_end', 'tool': 'web_search', 'node': current_node or "unknown"})
        msg_error = f'LangGraph error: {str(e)}' if is_en else f'LangGraph éŒ¯èª¤: {str(e)}'
        yield sse_event({'type': 'error', 'text': msg_error})
        if current_node:
            yield sse_event({'type': 'speaker_end', 'node': current_node})


# ============================================================
# SSE ä¸²æµæ¥å£
# ============================================================
@app.post("/debate")
async def start_debate(req: DebateRequest):
    """å•Ÿå‹• AI è¾¯è«–ä¸²æµ
    
    ä¸²æµæ¨¡å¼é¸æ“‡ï¼š
    1. USE_FAKE_STREAM=true æˆ–ç„¡ GROQ_API_KEY â†’ fake_debate_stream
    2. USE_LANGGRAPH=trueï¼ˆé è¨­ï¼‰â†’ langgraph_debate_streamï¼ˆPhase 3b, astream_eventsï¼‰
    3. USE_LANGGRAPH=false â†’ real_debate_streamï¼ˆPhase 2 å›é€€ï¼‰
    """
    
    # Debug log
    logger.info(f"ğŸš€ /debate API received: topic='{req.topic[:30]}...', max_rounds={req.max_rounds}, language={req.language}")
    
    if USE_FAKE_STREAM or not HAS_GROQ_KEY:
        stream_generator = fake_debate_stream(req.topic, req.max_rounds, req.language)
    elif USE_LANGGRAPH:
        stream_generator = langgraph_debate_stream(req.topic, req.max_rounds, req.language)
    else:
        stream_generator = real_debate_stream(req.topic, req.max_rounds, req.language)
    
    return StreamingResponse(
        stream_generator,
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        }
    )


# ============================================================
# åŸºç¤æ¥å£
# ============================================================
@app.get("/")
async def root():
    return {
        "message": "Welcome to DebateAI API ğŸ­",
        "version": "0.4.0",
        "phase": "4",
        "docs": "/docs"
    }


@app.get("/health")
async def health():
    from app.supabase_client import is_supabase_enabled
    return {
        "status": "healthy",
        "version": "0.4.0",
        "phase": "4",
        "has_groq_key": HAS_GROQ_KEY,
        "use_fake_stream": USE_FAKE_STREAM,
        "use_langgraph": USE_LANGGRAPH,
        "model": GROQ_MODEL if HAS_GROQ_KEY else None,
        "supabase_enabled": is_supabase_enabled(),
        "note": "Phase 4: Supabase debate history + i18n"
    }


# ============================================================
# Debate History Endpoints (Phase 4)
# ============================================================
class SaveDebateRequest(BaseModel):
    topic: str
    messages: list
    max_rounds: int = 3
    rounds_completed: int = 0


@app.post("/debate/save")
async def save_debate_endpoint(req: SaveDebateRequest):
    """å„²å­˜è¾¯è«–åˆ° Supabase"""
    from app.services.debate_service import save_debate
    
    debate_id = await save_debate(
        topic=req.topic,
        messages=req.messages,
        max_rounds=req.max_rounds,
        rounds_completed=req.rounds_completed
    )
    
    if debate_id:
        return {"success": True, "debate_id": debate_id}
    else:
        return {"success": False, "error": "Failed to save debate"}


@app.get("/debate/history")
async def get_history_endpoint(limit: int = 5):
    """å–å¾—æœ€è¿‘è¾¯è«–åˆ—è¡¨ (ç”¨æ–¼ sidebar)"""
    from app.services.debate_service import get_recent_debates
    
    debates = await get_recent_debates(limit=limit)
    return {"debates": debates}


@app.get("/debate/history/list")
async def get_history_paginated_endpoint(page: int = 1, page_size: int = 20):
    """åˆ†é å–å¾—è¾¯è«–åˆ—è¡¨"""
    from app.services.debate_service import get_debates_paginated
    
    result = await get_debates_paginated(page=page, page_size=page_size)
    return result


@app.get("/debate/history/{debate_id}")
async def get_debate_detail_endpoint(debate_id: str):
    """å–å¾—å–®ä¸€è¾¯è«–è©³ç´°å…§å®¹"""
    from app.services.debate_service import get_debate_by_id
    
    debate = await get_debate_by_id(debate_id)
    
    if debate:
        return debate
    else:
        from fastapi import HTTPException
        raise HTTPException(status_code=404, detail="Debate not found")
