"""
DebateAI Backend - FastAPI æ‡‰ç”¨

Phase 3b: LangGraph astream_events + æœå°‹å·¥å…·
- langgraph_debate_stream() ä½¿ç”¨ debate_graph.astream_events(version="v2")
- web_search_tool æä¾› Tavily + DuckDuckGo ä¸‰å±¤å®¹éŒ¯æœå°‹
- on_tool_start / on_tool_end äº‹ä»¶æ­£ç¢ºè§¸ç™¼å‰ç«¯æœå°‹æŒ‡ç¤ºå™¨
"""

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from dotenv import load_dotenv
import asyncio
import json
import re
import os

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

app = FastAPI(title="DebateAI API", version="0.3.1")


# ============================================================
# ç’°å¢ƒè®Šæ•¸
# ============================================================
USE_FAKE_STREAM = os.getenv("USE_FAKE_STREAM", "false").lower() == "true"
USE_LANGGRAPH = os.getenv("USE_LANGGRAPH", "true").lower() == "true"  # Phase 3b: é è¨­ä½¿ç”¨ LangGraph + astream_events
GROQ_API_KEY = os.getenv("GROQ_API_KEY", "")
HAS_GROQ_KEY = bool(GROQ_API_KEY and len(GROQ_API_KEY) > 10)
GROQ_MODEL = os.getenv("GROQ_MODEL", "llama-3.1-8b-instant")


# ============================================================
# Regex CORS Middleware
# ============================================================
class RegexCORSMiddleware(CORSMiddleware):
    """æ”¯æ´ regex åŒ¹é…çš„ CORS Middleware"""
    def is_allowed_origin(self, origin: str) -> bool:
        if not origin:
            return False
        if origin.startswith("http://localhost"):
            return True
        if re.match(r"https://.*\.pages\.dev$", origin):
            return True
        if re.match(r"https://.*\.ggff\.net$", origin):
            return True
        allowed = os.getenv("ALLOWED_ORIGINS", "")
        if allowed and allowed != "*":
            allowed_list = [o.strip() for o in allowed.split(",") if o.strip()]
            if origin in allowed_list:
                return True
        return super().is_allowed_origin(origin)

app.add_middleware(
    RegexCORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["Content-Type", "Authorization"],
    allow_credentials=True,
)


# ============================================================
# è«‹æ±‚æ¨¡å‹
# ============================================================
class DebateRequest(BaseModel):
    topic: str
    max_rounds: int = 3


# ============================================================
# SSE è¼”åŠ©å‡½æ•¸
# ============================================================
def sse_event(data: dict) -> str:
    """ç”Ÿæˆ SSE äº‹ä»¶æ ¼å¼"""
    return f"data: {json.dumps(data)}\n\n"


# ============================================================
# Fake SSE ä¸²æµï¼ˆFallbackï¼‰
# ============================================================
async def fake_debate_stream(topic: str, max_rounds: int = 3):
    """Phase 1 æ¸¬è©¦ç”¨ï¼šæ¨¡æ“¬ AI è¾¯è«–"""
    
    yield sse_event({'type': 'status', 'text': 'âš¡ [FAKE MODE] æ­£åœ¨å–šé†’æ¨¡æ“¬å¼•æ“...'})
    await asyncio.sleep(0.3)
    
    yield sse_event({'type': 'status', 'text': 'ğŸ”¥ æ¨¡æ“¬å¼•æ“å·²å°±ç·’ï¼'})
    
    for round_num in range(1, max_rounds + 1):
        # Optimist
        yield sse_event({'type': 'speaker', 'node': 'optimist', 'text': f'ç¬¬ {round_num} è¼ª'})
        
        optimist_text = f"é—œæ–¼ã€Œ{topic}ã€ï¼Œæˆ‘èªç‚ºé€™æ˜¯å……æ»¿æ©Ÿæœƒçš„ï¼ç§‘æŠ€é€²æ­¥ç¸½æ˜¯å¸¶ä¾†æ–°çš„å¯èƒ½æ€§ã€‚"
        for char in optimist_text:
            yield sse_event({'type': 'token', 'node': 'optimist', 'text': char})
            await asyncio.sleep(0.02)
        
        yield sse_event({'type': 'speaker_end', 'node': 'optimist'})
        
        # Skeptic
        yield sse_event({'type': 'speaker', 'node': 'skeptic', 'text': f'ç¬¬ {round_num} è¼ª'})
        
        skeptic_text = f"ç„¶è€Œï¼Œæˆ‘å€‘å¿…é ˆè¬¹æ…çœ‹å¾…ã€Œ{topic}ã€ã€‚ç›²ç›®æ¨‚è§€å¯èƒ½å°è‡´å¿½è¦–é¢¨éšªã€‚"
        for char in skeptic_text:
            yield sse_event({'type': 'token', 'node': 'skeptic', 'text': char})
            await asyncio.sleep(0.02)
        
        yield sse_event({'type': 'speaker_end', 'node': 'skeptic'})
    
    yield sse_event({'type': 'complete', 'text': f'âœ… [FAKE] è¾¯è«–çµæŸï¼å…± {max_rounds} è¼ªã€‚'})


# ============================================================
# çœŸå¯¦ LLM ä¸²æµ
# ============================================================
async def real_debate_stream(topic: str, max_rounds: int = 3):
    """Phase 2: çœŸæ­£çš„ Token-Level ä¸²æµ"""
    from app.graph import (
        get_llm, 
        create_initial_state, 
        build_prompt, 
        update_state_after_speaker
    )
    
    yield sse_event({'type': 'status', 'text': 'âš¡ æ­£åœ¨å–šé†’ AI è¾¯è«–å¼•æ“...'})
    
    # åˆå§‹åŒ–
    state = create_initial_state(topic, max_rounds)
    
    try:
        llm = get_llm()
        yield sse_event({'type': 'status', 'text': f'ğŸ”¥ ä½¿ç”¨æ¨¡å‹: {GROQ_MODEL}'})
    except Exception as e:
        yield sse_event({'type': 'error', 'text': f'LLM åˆå§‹åŒ–å¤±æ•—: {str(e)}'})
        return
    
    # è¾¯è«–å¾ªç’°
    while state['current_speaker'] != 'end':
        speaker = state['current_speaker']
        round_num = state['round_count'] + 1
        
        # ç™¼é€ speaker é–‹å§‹äº‹ä»¶
        yield sse_event({'type': 'speaker', 'node': speaker, 'text': f'ç¬¬ {round_num} è¼ª'})
        
        # å»ºæ§‹ prompt
        messages = build_prompt(state, speaker)
        
        # ç›´æ¥å‘¼å« llm.astream() å¯¦ç¾ token ä¸²æµ
        full_content = ""
        try:
            async for chunk in llm.astream(messages):
                if chunk.content:
                    full_content += chunk.content
                    yield sse_event({'type': 'token', 'node': speaker, 'text': chunk.content})
        except Exception as e:
            yield sse_event({'type': 'error', 'text': f'LLM ä¸²æµä¸­æ–·: {str(e)}'})
            yield sse_event({'type': 'speaker_end', 'node': speaker})
            yield sse_event({'type': 'complete', 'text': 'âŒ è¾¯è«–å› éŒ¯èª¤è€Œä¸­æ–·'})
            return
        
        # ç™¼é€ speaker çµæŸäº‹ä»¶
        yield sse_event({'type': 'speaker_end', 'node': speaker})
        
        # æ›´æ–°ç‹€æ…‹
        if full_content:
            state = update_state_after_speaker(state, speaker, full_content)
        else:
            yield sse_event({'type': 'error', 'text': 'LLM è¿”å›ç©ºå›æ‡‰'})
            break
    
    rounds_completed = state['round_count']
    yield sse_event({'type': 'complete', 'text': f'âœ… è¾¯è«–å®Œæˆï¼å…±é€²è¡Œäº† {rounds_completed} è¼ªç²¾å½©äº¤é‹’ã€‚'})
# ============================================================
# LangGraph StateGraph ä¸²æµï¼ˆPhase 3b - astream_eventsï¼‰
# ============================================================
async def langgraph_debate_stream(topic: str, max_rounds: int = 3):
    """Phase 3b: ä½¿ç”¨ astream_events å¯¦ç¾å·¥å…·äº‹ä»¶ä¸²æµ
    
    ä½¿ç”¨ astream_events è€Œé astream(stream_mode="messages")
    å¯ä»¥æ•æ‰ on_tool_start å’Œ on_tool_end äº‹ä»¶
    """
    from app.graph import debate_graph, create_initial_state
    
    yield sse_event({'type': 'status', 'text': 'âš¡ æ­£åœ¨å–šé†’ AI è¾¯è«–å¼•æ“...'})
    yield sse_event({'type': 'status', 'text': f'ğŸ”¥ ä½¿ç”¨æ¨¡å‹: {GROQ_MODEL} (LangGraph + Tools)'})
    
    state = create_initial_state(topic, max_rounds)
    
    current_node = None
    round_count = 0
    current_tool_query = None
    
    try:
        async for event in debate_graph.astream_events(
            state,
            version="v2"
        ):
            event_type = event.get("event")
            
            # ç¯€é»é–‹å§‹
            if event_type == "on_chain_start":
                name = event.get("name", "")
                if name in ("optimist", "skeptic"):
                    if current_node and current_node != name:
                        yield sse_event({'type': 'speaker_end', 'node': current_node})
                        if current_node == "skeptic":
                            round_count += 1
                    
                    current_node = name
                    display_round = round_count + 1
                    yield sse_event({
                        'type': 'speaker',
                        'node': name,
                        'text': f'ç¬¬ {display_round} è¼ª'
                    })
            
            # å·¥å…·é–‹å§‹
            elif event_type == "on_tool_start":
                tool_input = event.get("data", {}).get("input", {})
                query = tool_input.get("query", "æœªçŸ¥æŸ¥è©¢") if isinstance(tool_input, dict) else str(tool_input)
                current_tool_query = query
                yield sse_event({
                    'type': 'tool_start',
                    'tool': 'web_search',
                    'query': query,
                    'node': current_node or "unknown"
                })
            
            # å·¥å…·çµæŸï¼ˆæ­£å¸¸æˆ–éŒ¯èª¤ï¼‰
            elif event_type in ("on_tool_end", "on_tool_error"):
                yield sse_event({
                    'type': 'tool_end',
                    'tool': 'web_search',
                    'node': current_node or "unknown"
                })
                current_tool_query = None
            
            # LLM Token ä¸²æµ
            elif event_type == "on_chat_model_stream":
                chunk = event.get("data", {}).get("chunk")
                if chunk and hasattr(chunk, "content") and chunk.content:
                    yield sse_event({
                        'type': 'token',
                        'node': current_node or "unknown",
                        'text': chunk.content
                    })
        
        # çµæŸ
        if current_node:
            yield sse_event({'type': 'speaker_end', 'node': current_node})
            if current_node == "skeptic":
                round_count += 1
        
        yield sse_event({
            'type': 'complete',
            'text': f'âœ… è¾¯è«–å®Œæˆï¼å…±é€²è¡Œäº† {round_count} è¼ªç²¾å½©äº¤é‹’ã€‚'
        })
    
    except Exception as e:
        # ç¢ºä¿å·¥å…·æŒ‡ç¤ºå™¨è¢«æ¸…é™¤
        if current_tool_query:
            yield sse_event({'type': 'tool_end', 'tool': 'web_search', 'node': current_node or "unknown"})
        yield sse_event({'type': 'error', 'text': f'LangGraph éŒ¯èª¤: {str(e)}'})
        if current_node:
            yield sse_event({'type': 'speaker_end', 'node': current_node})


# ============================================================
# SSE ä¸²æµæ¥å£
# ============================================================
@app.post("/debate")
async def start_debate(req: DebateRequest):
    """å•Ÿå‹• AI è¾¯è«–ä¸²æµ
    
    ä¸²æµæ¨¡å¼é¸æ“‡ï¼š
    1. USE_FAKE_STREAM=true æˆ–ç„¡ GROQ_API_KEY â†’ fake_debate_stream
    2. USE_LANGGRAPH=trueï¼ˆé è¨­ï¼‰â†’ langgraph_debate_streamï¼ˆPhase 3b, astream_eventsï¼‰
    3. USE_LANGGRAPH=false â†’ real_debate_streamï¼ˆPhase 2 å›é€€ï¼‰
    """
    
    if USE_FAKE_STREAM or not HAS_GROQ_KEY:
        stream_generator = fake_debate_stream(req.topic, req.max_rounds)
    elif USE_LANGGRAPH:
        stream_generator = langgraph_debate_stream(req.topic, req.max_rounds)
    else:
        stream_generator = real_debate_stream(req.topic, req.max_rounds)
    
    return StreamingResponse(
        stream_generator,
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        }
    )


# ============================================================
# åŸºç¤æ¥å£
# ============================================================
@app.get("/")
async def root():
    return {
        "message": "Welcome to DebateAI API ğŸ­",
        "version": "0.3.1",
        "phase": "3b",
        "docs": "/docs"
    }


@app.get("/health")
async def health():
    return {
        "status": "healthy",
        "version": "0.3.1",
        "phase": "3b",
        "has_groq_key": HAS_GROQ_KEY,
        "use_fake_stream": USE_FAKE_STREAM,
        "use_langgraph": USE_LANGGRAPH,
        "model": GROQ_MODEL if HAS_GROQ_KEY else None,
        "note": "Phase 3b: astream_events + web_search_tool (Tavily/DuckDuckGo)"
    }
