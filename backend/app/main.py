"""
DebateAI Backend - FastAPI æ‡‰ç”¨

Phase 3a: LangGraph StateGraph ä¸²æµ
- langgraph_debate_stream() ä½¿ç”¨ debate_graph.astream(stream_mode="messages")
- USE_LANGGRAPH ç’°å¢ƒè®Šæ•¸æ§åˆ¶æ˜¯å¦ä½¿ç”¨ LangGraphï¼ˆé è¨­ trueï¼‰
- ä¿ç•™ real_debate_stream() ä½œç‚ºå›é€€æ–¹æ¡ˆï¼ˆUSE_LANGGRAPH=falseï¼‰
"""

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from dotenv import load_dotenv
import asyncio
import json
import re
import os

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

app = FastAPI(title="DebateAI API", version="0.3.0")


# ============================================================
# ç’°å¢ƒè®Šæ•¸
# ============================================================
USE_FAKE_STREAM = os.getenv("USE_FAKE_STREAM", "false").lower() == "true"
USE_LANGGRAPH = os.getenv("USE_LANGGRAPH", "true").lower() == "true"  # Phase 3a: é è¨­ä½¿ç”¨ LangGraph
GROQ_API_KEY = os.getenv("GROQ_API_KEY", "")
HAS_GROQ_KEY = bool(GROQ_API_KEY and len(GROQ_API_KEY) > 10)
GROQ_MODEL = os.getenv("GROQ_MODEL", "llama-3.1-8b-instant")


# ============================================================
# Regex CORS Middleware
# ============================================================
class RegexCORSMiddleware(CORSMiddleware):
    """æ”¯æ´ regex åŒ¹é…çš„ CORS Middleware"""
    def is_allowed_origin(self, origin: str) -> bool:
        if not origin:
            return False
        if origin.startswith("http://localhost"):
            return True
        if re.match(r"https://.*\.pages\.dev$", origin):
            return True
        if re.match(r"https://.*\.ggff\.net$", origin):
            return True
        allowed = os.getenv("ALLOWED_ORIGINS", "")
        if allowed and allowed != "*":
            allowed_list = [o.strip() for o in allowed.split(",") if o.strip()]
            if origin in allowed_list:
                return True
        return super().is_allowed_origin(origin)

app.add_middleware(
    RegexCORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["Content-Type", "Authorization"],
    allow_credentials=True,
)


# ============================================================
# è«‹æ±‚æ¨¡å‹
# ============================================================
class DebateRequest(BaseModel):
    topic: str
    max_rounds: int = 3


# ============================================================
# SSE è¼”åŠ©å‡½æ•¸
# ============================================================
def sse_event(data: dict) -> str:
    """ç”Ÿæˆ SSE äº‹ä»¶æ ¼å¼"""
    return f"data: {json.dumps(data)}\n\n"


# ============================================================
# Fake SSE ä¸²æµï¼ˆFallbackï¼‰
# ============================================================
async def fake_debate_stream(topic: str, max_rounds: int = 3):
    """Phase 1 æ¸¬è©¦ç”¨ï¼šæ¨¡æ“¬ AI è¾¯è«–"""
    
    yield sse_event({'type': 'status', 'text': 'âš¡ [FAKE MODE] æ­£åœ¨å–šé†’æ¨¡æ“¬å¼•æ“...'})
    await asyncio.sleep(0.3)
    
    yield sse_event({'type': 'status', 'text': 'ğŸ”¥ æ¨¡æ“¬å¼•æ“å·²å°±ç·’ï¼'})
    
    for round_num in range(1, max_rounds + 1):
        # Optimist
        yield sse_event({'type': 'speaker', 'node': 'optimist', 'text': f'ç¬¬ {round_num} è¼ª'})
        
        optimist_text = f"é—œæ–¼ã€Œ{topic}ã€ï¼Œæˆ‘èªç‚ºé€™æ˜¯å……æ»¿æ©Ÿæœƒçš„ï¼ç§‘æŠ€é€²æ­¥ç¸½æ˜¯å¸¶ä¾†æ–°çš„å¯èƒ½æ€§ã€‚"
        for char in optimist_text:
            yield sse_event({'type': 'token', 'node': 'optimist', 'text': char})
            await asyncio.sleep(0.02)
        
        yield sse_event({'type': 'speaker_end', 'node': 'optimist'})
        
        # Skeptic
        yield sse_event({'type': 'speaker', 'node': 'skeptic', 'text': f'ç¬¬ {round_num} è¼ª'})
        
        skeptic_text = f"ç„¶è€Œï¼Œæˆ‘å€‘å¿…é ˆè¬¹æ…çœ‹å¾…ã€Œ{topic}ã€ã€‚ç›²ç›®æ¨‚è§€å¯èƒ½å°è‡´å¿½è¦–é¢¨éšªã€‚"
        for char in skeptic_text:
            yield sse_event({'type': 'token', 'node': 'skeptic', 'text': char})
            await asyncio.sleep(0.02)
        
        yield sse_event({'type': 'speaker_end', 'node': 'skeptic'})
    
    yield sse_event({'type': 'complete', 'text': f'âœ… [FAKE] è¾¯è«–çµæŸï¼å…± {max_rounds} è¼ªã€‚'})


# ============================================================
# çœŸå¯¦ LLM ä¸²æµ
# ============================================================
async def real_debate_stream(topic: str, max_rounds: int = 3):
    """Phase 2: çœŸæ­£çš„ Token-Level ä¸²æµ"""
    from app.graph import (
        get_llm, 
        create_initial_state, 
        build_prompt, 
        update_state_after_speaker
    )
    
    yield sse_event({'type': 'status', 'text': 'âš¡ æ­£åœ¨å–šé†’ AI è¾¯è«–å¼•æ“...'})
    
    # åˆå§‹åŒ–
    state = create_initial_state(topic, max_rounds)
    
    try:
        llm = get_llm()
        yield sse_event({'type': 'status', 'text': f'ğŸ”¥ ä½¿ç”¨æ¨¡å‹: {GROQ_MODEL}'})
    except Exception as e:
        yield sse_event({'type': 'error', 'text': f'LLM åˆå§‹åŒ–å¤±æ•—: {str(e)}'})
        return
    
    # è¾¯è«–å¾ªç’°
    while state['current_speaker'] != 'end':
        speaker = state['current_speaker']
        round_num = state['round_count'] + 1
        
        # ç™¼é€ speaker é–‹å§‹äº‹ä»¶
        yield sse_event({'type': 'speaker', 'node': speaker, 'text': f'ç¬¬ {round_num} è¼ª'})
        
        # å»ºæ§‹ prompt
        messages = build_prompt(state, speaker)
        
        # ç›´æ¥å‘¼å« llm.astream() å¯¦ç¾ token ä¸²æµ
        full_content = ""
        try:
            async for chunk in llm.astream(messages):
                if chunk.content:
                    full_content += chunk.content
                    yield sse_event({'type': 'token', 'node': speaker, 'text': chunk.content})
        except Exception as e:
            yield sse_event({'type': 'error', 'text': f'LLM ä¸²æµä¸­æ–·: {str(e)}'})
            yield sse_event({'type': 'speaker_end', 'node': speaker})
            yield sse_event({'type': 'complete', 'text': 'âŒ è¾¯è«–å› éŒ¯èª¤è€Œä¸­æ–·'})
            return
        
        # ç™¼é€ speaker çµæŸäº‹ä»¶
        yield sse_event({'type': 'speaker_end', 'node': speaker})
        
        # æ›´æ–°ç‹€æ…‹
        if full_content:
            state = update_state_after_speaker(state, speaker, full_content)
        else:
            yield sse_event({'type': 'error', 'text': 'LLM è¿”å›ç©ºå›æ‡‰'})
            break
    
    rounds_completed = state['round_count']
    yield sse_event({'type': 'complete', 'text': f'âœ… è¾¯è«–å®Œæˆï¼å…±é€²è¡Œäº† {rounds_completed} è¼ªç²¾å½©äº¤é‹’ã€‚'})


# ============================================================
# LangGraph StateGraph ä¸²æµï¼ˆPhase 3aï¼‰
# ============================================================
async def langgraph_debate_stream(topic: str, max_rounds: int = 3):
    """Phase 3a: ä½¿ç”¨ LangGraph StateGraph ä¸²æµ
    
    âš ï¸ é—œéµé©—è­‰é»ï¼š
    1. tokens æ˜¯å¦é€ä¸€æ¨é€ï¼ˆæ‰“å­—æ©Ÿæ•ˆæœï¼‰
    2. metadata["langgraph_node"] æ˜¯å¦æ­£ç¢ºæä¾›ç™¼è¨€è€…è³‡è¨Š
    3. è¼ªæ¬¡è¨ˆç®—æ˜¯å¦æ­£ç¢ºï¼ˆç„¡ off-by-oneï¼‰
    """
    from app.graph import debate_graph, create_initial_state
    
    yield sse_event({'type': 'status', 'text': 'âš¡ æ­£åœ¨å–šé†’ AI è¾¯è«–å¼•æ“...'})
    yield sse_event({'type': 'status', 'text': f'ğŸ”¥ ä½¿ç”¨æ¨¡å‹: {GROQ_MODEL} (LangGraph)'})
    
    state = create_initial_state(topic, max_rounds)
    
    current_node = None
    round_count = 0  # ç¨ç«‹è¿½è¹¤è¼ªæ¬¡ï¼Œskeptic ç™¼è¨€çµæŸæ™‚ +1
    
    try:
        async for message, metadata in debate_graph.astream(
            state,
            stream_mode="messages"
        ):
            # âš ï¸ é˜²å‘†ï¼šmetadata["langgraph_node"] å¯èƒ½ç‚º None
            node = metadata.get("langgraph_node") if metadata else None
            if not node:
                continue  # è·³éç„¡æ•ˆäº‹ä»¶
            
            # ç¯€é»åˆ‡æ›æ™‚ç™¼é€ speaker äº‹ä»¶
            if node != current_node:
                # çµæŸå‰ä¸€å€‹ç¯€é»
                if current_node:
                    yield sse_event({'type': 'speaker_end', 'node': current_node})
                    # Skeptic ç™¼è¨€çµæŸå¾Œæ‰å¢åŠ è¼ªæ•¸
                    if current_node == "skeptic":
                        round_count += 1
                
                current_node = node
                
                # è¨ˆç®—é¡¯ç¤ºç”¨è¼ªæ¬¡ï¼ˆoptimist é–‹å ´æ™‚ç‚ºç¬¬ 1 è¼ªï¼‰
                display_round = round_count + 1
                yield sse_event({
                    'type': 'speaker',
                    'node': node,
                    'text': f'ç¬¬ {display_round} è¼ª'
                })
            
            # Token ä¸²æµ
            if hasattr(message, 'content') and message.content:
                yield sse_event({
                    'type': 'token',
                    'node': node,
                    'text': message.content
                })
        
        # æœ€å¾Œä¸€å€‹ç¯€é»çµæŸ
        if current_node:
            yield sse_event({'type': 'speaker_end', 'node': current_node})
            if current_node == "skeptic":
                round_count += 1
        
        yield sse_event({
            'type': 'complete',
            'text': f'âœ… è¾¯è«–å®Œæˆï¼å…±é€²è¡Œäº† {round_count} è¼ªç²¾å½©äº¤é‹’ã€‚'
        })
    
    except Exception as e:
        yield sse_event({'type': 'error', 'text': f'LangGraph éŒ¯èª¤: {str(e)}'})
        if current_node:
            yield sse_event({'type': 'speaker_end', 'node': current_node})


# ============================================================
# SSE ä¸²æµæ¥å£
# ============================================================
@app.post("/debate")
async def start_debate(req: DebateRequest):
    """å•Ÿå‹• AI è¾¯è«–ä¸²æµ
    
    ä¸²æµæ¨¡å¼é¸æ“‡ï¼š
    1. USE_FAKE_STREAM=true æˆ–ç„¡ GROQ_API_KEY â†’ fake_debate_stream
    2. USE_LANGGRAPH=trueï¼ˆé è¨­ï¼‰â†’ langgraph_debate_streamï¼ˆPhase 3aï¼‰
    3. USE_LANGGRAPH=false â†’ real_debate_streamï¼ˆPhase 2 å›é€€ï¼‰
    """
    
    if USE_FAKE_STREAM or not HAS_GROQ_KEY:
        stream_generator = fake_debate_stream(req.topic, req.max_rounds)
    elif USE_LANGGRAPH:
        stream_generator = langgraph_debate_stream(req.topic, req.max_rounds)
    else:
        stream_generator = real_debate_stream(req.topic, req.max_rounds)
    
    return StreamingResponse(
        stream_generator,
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        }
    )


# ============================================================
# åŸºç¤æ¥å£
# ============================================================
@app.get("/")
async def root():
    return {
        "message": "Welcome to DebateAI API ğŸ­",
        "version": "0.3.0",
        "phase": "3a",
        "docs": "/docs"
    }


@app.get("/health")
async def health():
    return {
        "status": "healthy",
        "version": "0.3.0",
        "phase": "3a",
        "has_groq_key": HAS_GROQ_KEY,
        "use_fake_stream": USE_FAKE_STREAM,
        "use_langgraph": USE_LANGGRAPH,
        "model": GROQ_MODEL if HAS_GROQ_KEY else None,
        "note": "Phase 3a: Using LangGraph StateGraph for debate flow control"
    }
