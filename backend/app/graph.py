"""
DebateAI - è¾¯è«–å¼•æ“ç‹€æ…‹ç®¡ç†æ¨¡çµ„

Phase 3c: ä½¿ç”¨ LangGraph ToolNode å¯¦ç¾å·¥å…·èª¿ç”¨äº‹ä»¶è¿½è¹¤
- Agent ç¯€é»åªè² è²¬æ±ºç­–ï¼ˆè¿”å› AIMessageï¼Œå¯èƒ½åŒ…å« tool_callsï¼‰
- ToolNode ç¨ç«‹åŸ·è¡Œå·¥å…·ï¼ˆLangGraph è‡ªå‹•è¿½è¹¤ on_tool_start/on_tool_endï¼‰
- æ¢ä»¶é‚Šæ§åˆ¶æµç¨‹
"""

from typing import TypedDict, Literal, List, Annotated
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage
from langchain_core.tools import tool
from langchain_groq import ChatGroq
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
import os
import logging

logger = logging.getLogger(__name__)


# ============================================================
# ç‹€æ…‹å®šç¾©ï¼ˆPhase 3cï¼‰
# ============================================================
class DebateState(TypedDict):
    """è¾¯è«–ç‹€æ…‹
    
    âš ï¸ messages ä½¿ç”¨ add_messages è¨»è§£ï¼ŒLangGraph æœƒè‡ªå‹•åˆä½µæ–°èˆŠè¨Šæ¯
    âš ï¸ current_speaker æ–°å¢ "tools" å’Œ "tool_callback" é¸é …
    """
    messages: Annotated[List[BaseMessage], add_messages]
    topic: str
    current_speaker: Literal["optimist", "skeptic", "tools", "tool_callback", "moderator", "end"]
    round_count: int
    max_rounds: int
    tool_iterations: int  # Phase 3c: å·¥å…·è¿­ä»£è¨ˆæ•¸å™¨
    last_agent: Literal["optimist", "skeptic", ""]  # Phase 3c: è¨˜éŒ„ä¸Šä¸€å€‹ Agent


# ============================================================
# System Prompts
# ============================================================
OPTIMIST_SYSTEM = """ä½ æ˜¯ä¸€ä½å……æ»¿èªªæœåŠ›çš„ã€Œæ¨‚è§€è¾¯æ‰‹ã€ã€‚

è¦å‰‡ï¼š
1. æ¯æ¬¡å›æ‡‰é™ 2-3 å¥è©±ï¼Œç°¡çŸ­æœ‰åŠ›
2. å¼·èª¿æ©Ÿæœƒã€å„ªå‹¢ã€æ­£é¢å½±éŸ¿
3. å¦‚æœå°æ‰‹æå‡ºè³ªç–‘ï¼Œå¿…é ˆæ­£é¢åæ“Š
4. ç¦æ­¢èªªã€Œä½ èªªå¾—å°ã€ã€Œæˆ‘åŒæ„ã€ç­‰é€€è®“èªå¥
5. ä½¿ç”¨ç¹é«”ä¸­æ–‡å›æ‡‰
6. **å¦‚æœéœ€è¦æ•¸æ“šæˆ–äº‹å¯¦æ”¯æŒè«–é»ï¼Œè«‹ä½¿ç”¨ web_search_tool æŸ¥è©¢**

å¯ç”¨å·¥å…·ï¼š
- web_search_tool(query: str): æœå°‹æœ€æ–°è³‡è¨Šã€çµ±è¨ˆæ•¸æ“šæˆ–äº‹å¯¦
"""

SKEPTIC_SYSTEM = """ä½ æ˜¯ä¸€ä½é‚è¼¯åš´è¬¹çš„ã€Œæ‡·ç–‘è¾¯æ‰‹ã€ã€‚

è¦å‰‡ï¼š
1. æ¯æ¬¡å›æ‡‰é™ 2-3 å¥è©±ï¼Œç›´æ“Šè¦å®³
2. æŒ‡å‡ºé¢¨éšªã€æ¼æ´ã€è¢«å¿½è¦–çš„ä»£åƒ¹
3. è³ªç–‘å°æ‰‹çš„æ¨‚è§€å‡è¨­ï¼Œè¦æ±‚æå‡ºè­‰æ“š
4. ç¦æ­¢èªåŒå°æ–¹è§€é»ï¼Œä¿æŒæ‰¹åˆ¤ç«‹å ´
5. ä½¿ç”¨ç¹é«”ä¸­æ–‡å›æ‡‰
6. **å¦‚æœéœ€è¦æŸ¥è­‰å°æ‰‹çš„è«–é»ï¼Œè«‹ä½¿ç”¨ web_search_tool**

å¯ç”¨å·¥å…·ï¼š
- web_search_tool(query: str): æœå°‹æœ€æ–°è³‡è¨Šä»¥æŸ¥è­‰è«–é»
"""


# Phase 3d: Moderator System Prompts
MODERATOR_ROUND_SUMMARY = """ä½ æ˜¯ä¸€ä½ä¸­ç«‹çš„è¾¯è«–ä¸»æŒäººã€‚
è«‹é‡å°æœ¬è¼ªè¾¯è«–åšç°¡çŸ­ç¸½çµï¼ˆ80-120å­—ï¼‰ï¼š

æ ¼å¼ï¼š
### ğŸ”„ ç¬¬ {round} è¼ªå°çµ
**æ¨‚è§€è€…**: [æ ¸å¿ƒè«–é» 1 å¥è©±]
**æ‡·ç–‘è€…**: [æ ¸å¿ƒè«–é» 1 å¥è©±]
**åˆ†æ­§é»**: [æœ¬è¼ªæœ€ä¸»è¦çš„çˆ­è­° 1 å¥è©±]

è¦å‰‡ï¼š
1. ä½¿ç”¨ç¹é«”ä¸­æ–‡
2. ä¿æŒçµ•å°ä¸­ç«‹
3. ç°¡æ½”æœ‰åŠ›ï¼Œä¸è¶…é 120 å­—
"""

MODERATOR_FINAL_SUMMARY = """ä½ æ˜¯ä¸€ä½ä¸­ç«‹ã€å®¢è§€çš„è¾¯è«–ä¸»æŒäººã€‚
è«‹æ ¹æ“šå®Œæ•´è¾¯è«–ç”Ÿæˆæœ€çµ‚ç¸½çµå ±å‘Šï¼ˆ200-300å­—ï¼‰ï¼š

æ ¼å¼ï¼š
## ğŸ“Š è¾¯è«–ç¸½çµå ±å‘Š

### ğŸŸ¢ æ¨‚è§€è€…æ ¸å¿ƒè«–é»
- [è«–é» 1]
- [è«–é» 2]

### ğŸ”´ æ‡·ç–‘è€…æ ¸å¿ƒè«–é»
- [è«–é» 1]
- [è«–é» 2]

### âš–ï¸ é—œéµåˆ†æ­§é»
[é›™æ–¹æœ€ä¸»è¦çš„åˆ†æ­§æ˜¯ä»€éº¼ï¼Ÿ1-2 å¥è©±]

### ğŸ’¡ ç¶œåˆè©•ä¼°
[å®¢è§€åˆ†æé›™æ–¹è«–è­‰çš„å„ªåŠ£ï¼ŒæŒ‡å‡ºå“ªäº›è«–é»æ›´æœ‰èªªæœåŠ›ï¼Œ2-3 å¥è©±]

### ğŸ¯ çµè«–å»ºè­°
[çµ¦è®€è€…çš„å¯¦ç”¨å»ºè­°ï¼Œ1 å¥è©±]

è¦å‰‡ï¼š
1. ä½¿ç”¨ç¹é«”ä¸­æ–‡
2. ä¿æŒä¸­ç«‹å®¢è§€
3. ç¸½å­—æ•¸ 200-300 å­—
"""


# ============================================================
# å·¥å…·å®šç¾©ï¼ˆPhase 3b/3cï¼‰
# ============================================================

@tool
async def web_search_tool(query: str) -> str:
    """æœå°‹ç¶²è·¯è³‡æ–™ä»¥ç²å–æœ€æ–°è³‡è¨Šã€çµ±è¨ˆæ•¸æ“šæˆ–äº‹å¯¦ã€‚

    ç•¶éœ€è¦ä»¥ä¸‹æƒ…æ³æ™‚ä½¿ç”¨æ­¤å·¥å…·ï¼š
    - æœ€æ–°æ•¸æ“šæˆ–çµ±è¨ˆè³‡æ–™
    - å…·é«”äº‹ä»¶çš„æ—¥æœŸå’Œç´°ç¯€
    - ç§‘å­¸ç ”ç©¶çµæœ
    - å¸‚å ´è¶¨å‹¢æˆ–å•†æ¥­è³‡è¨Š

    Args:
        query: æœå°‹é—œéµå­—ï¼ˆç°¡æ½”æ˜ç¢ºï¼‰

    Returns:
        æ ¼å¼åŒ–çš„æœå°‹çµæœæ‘˜è¦
    """
    from app.tools.search import web_search
    
    logger.debug(f"web_search_tool called with query: {query}")
    result = await web_search(query)
    return result.get("formatted", "æœå°‹å¤±æ•—")


# å·¥å…·åˆ—è¡¨ï¼ˆç”¨æ–¼ ToolNodeï¼‰
tools = [web_search_tool]


# ============================================================
# LLM å·¥å» 
# ============================================================
def get_llm(bind_tools: bool = False):
    """å–å¾— LLM å¯¦ä¾‹

    Args:
        bind_tools: æ˜¯å¦ç¶å®šå·¥å…·ï¼ˆPhase 3cï¼‰
    """
    model_name = os.getenv("GROQ_MODEL", "llama-3.1-8b-instant")
    llm = ChatGroq(
        model=model_name,
        temperature=0.7,
        api_key=os.getenv("GROQ_API_KEY"),
        streaming=True
    )

    if bind_tools:
        return llm.bind_tools(tools)
    return llm


# ============================================================
# è¼”åŠ©å‡½æ•¸
# ============================================================
def format_messages(messages: List[BaseMessage], limit: int = 4) -> str:
    """æ ¼å¼åŒ–æœ€è¿‘çš„è¨Šæ¯æ­·å²"""
    recent = messages[-limit:] if len(messages) > limit else messages
    lines = []
    for m in recent:
        name = getattr(m, 'name', None) or m.__class__.__name__
        if hasattr(m, 'content') and m.content:
            lines.append(f"[{name}]: {m.content}")
    return "\n".join(lines) if lines else "(å°šç„¡å°è©±)"


def build_prompt(state: DebateState, speaker: str) -> List[BaseMessage]:
    """ç‚ºæŒ‡å®šç™¼è¨€è€…å»ºæ§‹ promptï¼ˆåªç”¨æ–¼é¦–æ¬¡èª¿ç”¨ï¼‰"""
    history = format_messages(state['messages'])
    round_num = state['round_count'] + 1
    
    if speaker == "optimist":
        system = OPTIMIST_SYSTEM
        if state['round_count'] == 0 and len(state['messages']) == 0:
            user_content = f"""è¾¯è«–ä¸»é¡Œï¼š{state['topic']}

é–‹å ´ç™½ï¼Œè«‹ä»¥æ¨‚è§€è€…èº«ä»½ç™¼è¨€ã€‚"""
        else:
            user_content = f"""è¾¯è«–ä¸»é¡Œï¼š{state['topic']}

ç¬¬ {round_num} è¼ªï¼Œè«‹ä»¥æ¨‚è§€è€…èº«ä»½ç™¼è¨€ã€‚

å°è©±æ­·å²ï¼š
{history}"""
    else:  # skeptic
        system = SKEPTIC_SYSTEM
        user_content = f"""è¾¯è«–ä¸»é¡Œï¼š{state['topic']}

ç¬¬ {round_num} è¼ªï¼Œè«‹ä»¥æ‡·ç–‘è€…èº«ä»½åé§æ¨‚è§€è€…çš„è«–é»ã€‚

å°è©±æ­·å²ï¼š
{history}"""
    
    return [
        SystemMessage(content=system),
        HumanMessage(content=user_content)
    ]


def update_state_after_speaker(state: DebateState, speaker: str, content: str) -> DebateState:
    """ç™¼è¨€çµæŸå¾Œæ›´æ–°ç‹€æ…‹ï¼ˆä¿ç•™ä¾› real_debate_stream ä½¿ç”¨ï¼‰"""
    new_messages = state['messages'] + [AIMessage(content=content, name=speaker)]
    
    if speaker == "optimist":
        return {
            **state,
            "messages": new_messages,
            "current_speaker": "skeptic"
        }
    else:
        new_round = state['round_count'] + 1
        next_speaker = "end" if new_round >= state['max_rounds'] else "optimist"
        return {
            **state,
            "messages": new_messages,
            "current_speaker": next_speaker,
            "round_count": new_round
        }


def create_initial_state(topic: str, max_rounds: int = 3) -> DebateState:
    """å»ºç«‹åˆå§‹ç‹€æ…‹ï¼ˆPhase 3cï¼‰"""
    return {
        "messages": [],
        "topic": topic,
        "current_speaker": "optimist",
        "round_count": 0,
        "max_rounds": max_rounds,
        "tool_iterations": 0,
        "last_agent": ""
    }


# ============================================================
# LangGraph StateGraphï¼ˆPhase 3c - ToolNode æ¶æ§‹ï¼‰
# ============================================================

MAX_TOOL_ITERATIONS = 3


async def optimist_node(state: DebateState) -> dict:
    """æ¨‚è§€è€…ç¯€é»ï¼ˆPhase 3c: åƒ…æ±ºç­–ï¼Œä¸åŸ·è¡Œå·¥å…·ï¼‰
    
    è¿”å›çš„ AIMessage å¯èƒ½åŒ…å« tool_callsï¼Œç”±æ¢ä»¶é‚Šæ±ºå®šä¸‹ä¸€æ­¥
    """
    logger.debug("optimist_node: entering")
    
    # æª¢æŸ¥æ˜¯å¦å·²é”åˆ°å·¥å…·è¿­ä»£ä¸Šé™
    tool_iterations = state.get('tool_iterations', 0)
    should_bind_tools = tool_iterations < MAX_TOOL_ITERATIONS
    
    llm = get_llm(bind_tools=should_bind_tools)
    logger.debug(f"optimist_node: tool_iterations={tool_iterations}, bind_tools={should_bind_tools}")
    
    # æª¢æŸ¥æ˜¯å¦å¾å·¥å…·å›èª¿è¿”å›ï¼ˆmessages ä¸­æœ‰ ToolMessageï¼‰
    messages = state.get('messages', [])
    if messages and isinstance(messages[-1], ToolMessage):
        # å¾å·¥å…·è¿”å›ï¼šæå–å·¥å…·çµæœä½œç‚ºæ–‡å­—
        tool_results = []
        for msg in reversed(messages[-6:]):
            if isinstance(msg, ToolMessage):
                tool_results.insert(0, f"[æœå°‹çµæœ]: {msg.content}")
        
        tool_context = "\n".join(tool_results) if tool_results else ""
        history = format_messages(messages)
        
        prompt_messages = [
            SystemMessage(content=OPTIMIST_SYSTEM),
            HumanMessage(content=f"""è¾¯è«–ä¸»é¡Œï¼š{state['topic']}

{tool_context}

è«‹æ ¹æ“šä»¥ä¸Šæœå°‹çµæœï¼Œä»¥æ¨‚è§€è€…èº«ä»½ç¹¼çºŒç™¼è¨€ã€‚ï¼ˆè«‹ç›´æ¥ç™¼è¨€ï¼Œä¸è¦å†æœå°‹ï¼‰

å°è©±æ­·å²ï¼š
{history}""")
        ]
    else:
        # é¦–æ¬¡èª¿ç”¨
        prompt_messages = build_prompt(state, "optimist")
    
    response = await llm.ainvoke(prompt_messages)
    logger.debug(f"optimist_node: response has tool_calls={bool(getattr(response, 'tool_calls', None))}")
    
    # æª¢æŸ¥æ˜¯å¦æœ‰å·¥å…·èª¿ç”¨
    has_tool_calls = hasattr(response, 'tool_calls') and response.tool_calls
    
    if has_tool_calls:
        # â„¹ï¸ Groq è¦æ±‚æ‰€æœ‰ AIMessage éƒ½æœ‰ name å±¬æ€§
        response_with_name = AIMessage(
            content=response.content or "",
            tool_calls=response.tool_calls,
            name="optimist"
        )
        return {
            "messages": [response_with_name],
            "current_speaker": "tools",
            "last_agent": "optimist"
        }
    else:
        # ç¢ºä¿ content æœ‰ name å±¬æ€§
        final_response = AIMessage(content=response.content or "(ç„¡å›æ‡‰)", name="optimist")
        return {
            "messages": [final_response],
            "current_speaker": "skeptic",
            "last_agent": "optimist",
            "tool_iterations": 0  # é‡ç½®
        }


async def skeptic_node(state: DebateState) -> dict:
    """æ‡·ç–‘è€…ç¯€é»ï¼ˆPhase 3c: åƒ…æ±ºç­–ï¼Œä¸åŸ·è¡Œå·¥å…·ï¼‰"""
    logger.debug("skeptic_node: entering")
    
    # æª¢æŸ¥æ˜¯å¦å·²é”åˆ°å·¥å…·è¿­ä»£ä¸Šé™
    tool_iterations = state.get('tool_iterations', 0)
    should_bind_tools = tool_iterations < MAX_TOOL_ITERATIONS
    
    llm = get_llm(bind_tools=should_bind_tools)
    logger.debug(f"skeptic_node: tool_iterations={tool_iterations}, bind_tools={should_bind_tools}")
    
    messages = state.get('messages', [])
    if messages and isinstance(messages[-1], ToolMessage):
        # å¾å·¥å…·è¿”å›ï¼šæå–å·¥å…·çµæœä½œç‚ºæ–‡å­—
        tool_results = []
        for msg in reversed(messages[-6:]):
            if isinstance(msg, ToolMessage):
                tool_results.insert(0, f"[æœå°‹çµæœ]: {msg.content}")
        
        tool_context = "\n".join(tool_results) if tool_results else ""
        history = format_messages(messages)
        
        prompt_messages = [
            SystemMessage(content=SKEPTIC_SYSTEM),
            HumanMessage(content=f"""è¾¯è«–ä¸»é¡Œï¼š{state['topic']}

{tool_context}

è«‹æ ¹æ“šä»¥ä¸Šæœå°‹çµæœï¼Œä»¥æ‡·ç–‘è€…èº«ä»½ç¹¼çºŒåé§ã€‚ï¼ˆè«‹ç›´æ¥ç™¼è¨€ï¼Œä¸è¦å†æœå°‹ï¼‰

å°è©±æ­·å²ï¼š
{history}""")
        ]
    else:
        prompt_messages = build_prompt(state, "skeptic")
    
    response = await llm.ainvoke(prompt_messages)
    logger.debug(f"skeptic_node: response has tool_calls={bool(getattr(response, 'tool_calls', None))}")
    
    has_tool_calls = hasattr(response, 'tool_calls') and response.tool_calls
    
    if has_tool_calls:
        # â„¹ï¸ Groq è¦æ±‚æ‰€æœ‰ AIMessage éƒ½æœ‰ name å±¬æ€§
        response_with_name = AIMessage(
            content=response.content or "",
            tool_calls=response.tool_calls,
            name="skeptic"
        )
        return {
            "messages": [response_with_name],
            "current_speaker": "tools",
            "last_agent": "skeptic"
        }
    else:
        # Phase 3d: å°å‘ Moderatorï¼ˆè€Œé Optimist æˆ– ENDï¼‰
        final_response = AIMessage(content=response.content or "(ç„¡å›æ‡‰)", name="skeptic")
        
        # âš ï¸ ä¸å†åœ¨æ­¤å¢åŠ è¼ªæ•¸ï¼Œäº¤ç”± Moderator è™•ç†
        return {
            "messages": [final_response],
            "current_speaker": "moderator",
            "last_agent": "skeptic",
            "tool_iterations": 0
        }


async def tool_callback_node(state: DebateState) -> dict:
    """å·¥å…·åŸ·è¡Œå¾Œçš„å›èª¿ç¯€é»
    
    æ±ºå®šè¿”å›å“ªå€‹ Agentï¼Œä¸¦æª¢æŸ¥è¿­ä»£é™åˆ¶
    """
    logger.debug("tool_callback_node: entering")
    iterations = state.get("tool_iterations", 0) + 1
    last_agent = state.get("last_agent", "optimist")
    
    logger.debug(f"tool_callback_node: iterations={iterations}, last_agent={last_agent}")
    
    # æª¢æŸ¥æ˜¯å¦è¶…éé™åˆ¶
    if iterations >= MAX_TOOL_ITERATIONS:
        logger.warning(f"tool_callback_node: max iterations reached, forcing return to {last_agent}")
        return {
            "current_speaker": last_agent,
            "tool_iterations": 0
        }
    
    return {
        "current_speaker": last_agent,
        "tool_iterations": iterations
    }


def should_continue(state: DebateState) -> str:
    """è·¯ç”±å‡½æ•¸ï¼šæ ¹æ“š current_speaker æ±ºå®šä¸‹ä¸€å€‹ç¯€é»"""
    speaker = state.get("current_speaker", "end")
    logger.debug(f"should_continue: current_speaker={speaker}")
    return speaker


# ============================================================
# Phase 3d: Moderator ç¯€é»
# ============================================================

async def moderator_node(state: DebateState) -> dict:
    """ä¸»æŒäººç¯€é»ï¼šç”Ÿæˆéšæ®µæ€§æˆ–æœ€çµ‚ç¸½çµ

    é‚è¼¯ï¼š
    - è¼ªæ¬¡ < max_rounds: éšæ®µæ€§ç¸½çµ â†’ è¿”å› optimist
    - è¼ªæ¬¡ = max_rounds: æœ€çµ‚ç¸½çµ â†’ è¿”å› end
    """
    logger.debug("moderator_node: entering")

    llm = get_llm(bind_tools=False)  # ä¸ç¶å®šå·¥å…·

    current_round = state.get("round_count", 0) + 1  # Moderator åŸ·è¡Œæ™‚é‚„æœª ++
    max_rounds = state.get("max_rounds", 3)
    is_final = (current_round >= max_rounds)

    # é¸æ“‡ Prompt
    if is_final:
        system_prompt = MODERATOR_FINAL_SUMMARY
        prompt_context = f"""è¾¯è«–ä¸»é¡Œï¼š{state['topic']}

å®Œæ•´è¾¯è«–è¨˜éŒ„ï¼š
{format_messages(state['messages'], limit=30)}

è«‹ç”Ÿæˆæœ€çµ‚ç¸½çµå ±å‘Šã€‚"""
    else:
        system_prompt = MODERATOR_ROUND_SUMMARY.format(round=current_round)

        # âš ï¸ åªæå–æœ¬è¼ªçš„ Optimist/Skeptic å°è©±ï¼ˆé¿å…åŒ…å«èˆŠçš„ Moderator ç¸½çµï¼‰
        recent_debate_msgs = [
            m for m in state['messages'][-8:]
            if getattr(m, 'name', None) in ("optimist", "skeptic")
        ]

        prompt_context = f"""è¾¯è«–ä¸»é¡Œï¼š{state['topic']}

æœ¬è¼ªå°è©±ï¼š
{format_messages(recent_debate_msgs, limit=6)}

è«‹ç”Ÿæˆç¬¬ {current_round} è¼ªå°çµã€‚"""

    prompt_messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=prompt_context)
    ]

    response = await llm.ainvoke(prompt_messages)
    final_response = AIMessage(
        content=response.content or "(ç„¡æ³•ç”Ÿæˆç¸½çµ)",
        name="moderator"
    )

    # æ±ºå®šä¸‹ä¸€æ­¥
    new_round = current_round  # Moderator å®Œæˆå¾Œæ‰æ›´æ–°è¼ªæ•¸
    next_speaker = "end" if is_final else "optimist"

    logger.debug(f"moderator_node: round={new_round}, next={next_speaker}")

    return {
        "messages": [final_response],
        "current_speaker": next_speaker,
        "round_count": new_round,  # âš ï¸ é‡è¦ï¼šModerator è² è²¬æ›´æ–°è¼ªæ•¸
        "tool_iterations": 0  # æ¸…é›¶å·¥å…·è¨ˆæ•¸ï¼Œè®“ä¸‹ä¸€è¼ªå¯ä»¥ä½¿ç”¨å·¥å…·
    }


# ============================================================
# å»ºç«‹ StateGraphï¼ˆPhase 3dï¼‰
# ============================================================

# å»ºç«‹ ToolNode
tool_node = ToolNode(tools)

# å»ºç«‹ StateGraph
_graph = StateGraph(DebateState)

# æ·»åŠ ç¯€é»
_graph.add_node("optimist", optimist_node)
_graph.add_node("skeptic", skeptic_node)
_graph.add_node("tools", tool_node)
_graph.add_node("tool_callback", tool_callback_node)
_graph.add_node("moderator", moderator_node)  # Phase 3d: æ–°å¢

# è¨­å®šå…¥å£é»
_graph.set_conditional_entry_point(
    should_continue,
    {
        "optimist": "optimist",
        "skeptic": "skeptic",
        "tools": "tools",
        "moderator": "moderator",  # Phase 3d: æ–°å¢
        "end": END
    }
)

# Optimist å¾Œçš„è·¯ç”±
_graph.add_conditional_edges(
    "optimist",
    should_continue,
    {
        "tools": "tools",
        "skeptic": "skeptic",
        "tool_callback": "tool_callback",
        "moderator": "moderator",  # Phase 3d: å®¹éŒ¯è·¯ç”±
        "end": END
    }
)

# Skeptic å¾Œçš„è·¯ç”±
_graph.add_conditional_edges(
    "skeptic",
    should_continue,
    {
        "tools": "tools",
        "optimist": "optimist",
        "tool_callback": "tool_callback",
        "moderator": "moderator",  # Phase 3d: ä¸»è¦è·¯ç”±
        "end": END
    }
)

# Tool åŸ·è¡Œå¾Œé€²å…¥å›èª¿
_graph.add_edge("tools", "tool_callback")

# Tool å›èª¿å¾Œè¿”å› Agent
_graph.add_conditional_edges(
    "tool_callback",
    should_continue,
    {
        "optimist": "optimist",
        "skeptic": "skeptic",
        "moderator": "moderator",  # Phase 3d: å®¹éŒ¯è·¯ç”±
        "end": END
    }
)

# Moderator å¾Œçš„è·¯ç”±ï¼ˆPhase 3dï¼‰
_graph.add_conditional_edges(
    "moderator",
    should_continue,
    {
        "optimist": "optimist",  # æœªæ»¿ 3 è¼ªï¼Œç¹¼çºŒè¾¯è«–
        "end": END               # å·²æ»¿ 3 è¼ªï¼ŒçµæŸ
    }
)

# ç·¨è­¯ç‚ºå¯åŸ·è¡Œçš„ graph
debate_graph = _graph.compile()

logger.info("debate_graph compiled successfully (Phase 3d: Moderator Agent)")

