"""
DebateAI - è¾¯è«–å¼•æ“ç‹€æ…‹ç®¡ç†æ¨¡çµ„

Phase 3c: ä½¿ç”¨ LangGraph ToolNode å¯¦ç¾å·¥å…·èª¿ç”¨äº‹ä»¶è¿½è¹¤
- Agent ç¯€é»åªè² è²¬æ±ºç­–ï¼ˆè¿”å› AIMessageï¼Œå¯èƒ½åŒ…å« tool_callsï¼‰
- ToolNode ç¨ç«‹åŸ·è¡Œå·¥å…·ï¼ˆLangGraph è‡ªå‹•è¿½è¹¤ on_tool_start/on_tool_endï¼‰
- æ¢ä»¶é‚Šæ§åˆ¶æµç¨‹
"""

from typing import TypedDict, Literal, List, Annotated
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage
from langchain_core.tools import tool
from langchain_groq import ChatGroq
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
import os
import logging

logger = logging.getLogger(__name__)


# ============================================================
# ç‹€æ…‹å®šç¾©ï¼ˆPhase 3cï¼‰
# ============================================================
class DebateState(TypedDict):
    """è¾¯è«–ç‹€æ…‹
    
    âš ï¸ messages ä½¿ç”¨ add_messages è¨»è§£ï¼ŒLangGraph æœƒè‡ªå‹•åˆä½µæ–°èˆŠè¨Šæ¯
    âš ï¸ current_speaker æ–°å¢ "tools" å’Œ "tool_callback" é¸é …
    """
    messages: Annotated[List[BaseMessage], add_messages]
    topic: str
    current_speaker: Literal["optimist", "skeptic", "tools", "tool_callback", "moderator", "end"]
    round_count: int
    max_rounds: int
    tool_iterations: int  # Phase 3c: å·¥å…·è¿­ä»£è¨ˆæ•¸å™¨
    last_agent: Literal["optimist", "skeptic", ""]  # Phase 3c: è¨˜éŒ„ä¸Šä¸€å€‹ Agent


# ============================================================
# èªè¨€è¨­å®š
# ============================================================
current_language = "zh"  # é è¨­ç¹é«”ä¸­æ–‡

def set_language(lang: str):
    """è¨­å®šèªè¨€ï¼š'zh' (ç¹é«”ä¸­æ–‡) æˆ– 'en' (English)"""
    global current_language
    current_language = lang if lang in ("zh", "en") else "zh"
    logger.info(f"ğŸŒ Language set to: {current_language}")

def get_language() -> str:
    """å–å¾—ç•¶å‰èªè¨€è¨­å®š"""
    return current_language


# ============================================================
# System Prompts - ä¸­æ–‡ç‰ˆ
# ============================================================
OPTIMIST_SYSTEM_ZH = """ä½ æ˜¯ä¸€ä½å……æ»¿èªªæœåŠ›çš„ã€Œæ¨‚è§€è¾¯æ‰‹ã€ã€‚

è¦å‰‡ï¼š
1. æ¯æ¬¡å›æ‡‰é™ 2-3 å¥è©±ï¼Œç°¡çŸ­æœ‰åŠ›
2. å¼·èª¿æ©Ÿæœƒã€å„ªå‹¢ã€æ­£é¢å½±éŸ¿
3. å¦‚æœå°æ‰‹æå‡ºè³ªç–‘ï¼Œå¿…é ˆæ­£é¢åæ“Š
4. ç¦æ­¢èªªã€Œä½ èªªå¾—å°ã€ã€Œæˆ‘åŒæ„ã€ç­‰é€€è®“èªå¥
5. ä½¿ç”¨ç¹é«”ä¸­æ–‡å›æ‡‰
6. **å¦‚æœéœ€è¦æ•¸æ“šæˆ–äº‹å¯¦æ”¯æŒè«–é»ï¼Œè«‹ä½¿ç”¨ web_search_tool æŸ¥è©¢**

å¯ç”¨å·¥å…·ï¼š
- web_search_tool(query: str): æœå°‹æœ€æ–°è³‡è¨Šã€çµ±è¨ˆæ•¸æ“šæˆ–äº‹å¯¦
"""

SKEPTIC_SYSTEM_ZH = """ä½ æ˜¯ä¸€ä½é‚è¼¯åš´è¬¹çš„ã€Œæ‡·ç–‘è¾¯æ‰‹ã€ã€‚

è¦å‰‡ï¼š
1. æ¯æ¬¡å›æ‡‰é™ 2-3 å¥è©±ï¼Œç›´æ“Šè¦å®³
2. æŒ‡å‡ºé¢¨éšªã€æ¼æ´ã€è¢«å¿½è¦–çš„ä»£åƒ¹
3. è³ªç–‘å°æ‰‹çš„æ¨‚è§€å‡è¨­ï¼Œè¦æ±‚æå‡ºè­‰æ“š
4. ç¦æ­¢èªåŒå°æ–¹è§€é»ï¼Œä¿æŒæ‰¹åˆ¤ç«‹å ´
5. ä½¿ç”¨ç¹é«”ä¸­æ–‡å›æ‡‰
6. **å¦‚æœéœ€è¦æŸ¥è­‰å°æ‰‹çš„è«–é»ï¼Œè«‹ä½¿ç”¨ web_search_tool**

å¯ç”¨å·¥å…·ï¼š
- web_search_tool(query: str): æœå°‹æœ€æ–°è³‡è¨Šä»¥æŸ¥è­‰è«–é»
"""

MODERATOR_ROUND_SUMMARY_ZH = """ä½ æ˜¯ä¸€ä½ä¸­ç«‹çš„è¾¯è«–ä¸»æŒäººã€‚
è«‹é‡å°æœ¬è¼ªè¾¯è«–åšç°¡çŸ­ç¸½çµï¼ˆ80-120å­—ï¼‰ï¼š

æ ¼å¼ï¼š
### ğŸ”„ ç¬¬ {round} è¼ªå°çµ
**æ¨‚è§€è€…**: [æ ¸å¿ƒè«–é» 1 å¥è©±]
**æ‡·ç–‘è€…**: [æ ¸å¿ƒè«–é» 1 å¥è©±]
**åˆ†æ­§é»**: [æœ¬è¼ªæœ€ä¸»è¦çš„çˆ­è­° 1 å¥è©±]

è¦å‰‡ï¼š
1. ä½¿ç”¨ç¹é«”ä¸­æ–‡
2. ä¿æŒçµ•å°ä¸­ç«‹
3. ç°¡æ½”æœ‰åŠ›ï¼Œä¸è¶…é 120 å­—
"""

MODERATOR_FINAL_SUMMARY_ZH = """ä½ æ˜¯ä¸€ä½ä¸­ç«‹ã€å®¢è§€çš„è¾¯è«–ä¸»æŒäººã€‚
è«‹æ ¹æ“šå®Œæ•´è¾¯è«–ç”Ÿæˆæœ€çµ‚ç¸½çµå ±å‘Šï¼ˆ200-300å­—ï¼‰ï¼š

æ ¼å¼ï¼š
## ğŸ“Š è¾¯è«–ç¸½çµå ±å‘Š

### ğŸŸ¢ æ¨‚è§€è€…æ ¸å¿ƒè«–é»
- [è«–é» 1]
- [è«–é» 2]

### ğŸ”´ æ‡·ç–‘è€…æ ¸å¿ƒè«–é»
- [è«–é» 1]
- [è«–é» 2]

### âš–ï¸ é—œéµåˆ†æ­§é»
[é›™æ–¹æœ€ä¸»è¦çš„åˆ†æ­§æ˜¯ä»€éº¼ï¼Ÿ1-2 å¥è©±]

### ğŸ’¡ ç¶œåˆè©•ä¼°
[å®¢è§€åˆ†æé›™æ–¹è«–è­‰çš„å„ªåŠ£ï¼ŒæŒ‡å‡ºå“ªäº›è«–é»æ›´æœ‰èªªæœåŠ›ï¼Œ2-3 å¥è©±]

### ğŸ¯ çµè«–å»ºè­°
[çµ¦è®€è€…çš„å¯¦ç”¨å»ºè­°ï¼Œ1 å¥è©±]

è¦å‰‡ï¼š
1. ä½¿ç”¨ç¹é«”ä¸­æ–‡
2. ä¿æŒä¸­ç«‹å®¢è§€
3. ç¸½å­—æ•¸ 200-300 å­—
"""


# ============================================================
# System Prompts - è‹±æ–‡ç‰ˆ
# ============================================================
OPTIMIST_SYSTEM_EN = """You are a persuasive "Optimist Debater".

**IMPORTANT: You MUST respond in English only, regardless of the topic's language.**

Rules:
1. Keep responses to 2-3 sentences, short and powerful
2. Emphasize opportunities, advantages, and positive impacts
3. If challenged, respond with strong counter-arguments
4. Never say "you're right" or "I agree" - no concessions
5. **Always respond in English, even if the topic is in another language**
6. **Use web_search_tool to find data or facts to support your arguments**

Available tools:
- web_search_tool(query: str): Search for latest information, statistics, or facts
"""

SKEPTIC_SYSTEM_EN = """You are a logical "Skeptic Debater".

**IMPORTANT: You MUST respond in English only, regardless of the topic's language.**

Rules:
1. Keep responses to 2-3 sentences, hit the key points
2. Point out risks, flaws, and overlooked costs
3. Challenge opponent's optimistic assumptions, demand evidence
4. Never agree with opponent's view, maintain critical stance
5. **Always respond in English, even if the topic is in another language**
6. **Use web_search_tool to verify opponent's claims**

Available tools:
- web_search_tool(query: str): Search for latest information to verify claims
"""

MODERATOR_ROUND_SUMMARY_EN = """You are a neutral debate moderator.

**IMPORTANT: Respond in English only, even if the debate was in another language.**

Please provide a brief summary of this round (80-120 words):

Format:
### ğŸ”„ Round {round} Summary
**Optimist**: [Core argument in 1 sentence]
**Skeptic**: [Core argument in 1 sentence]
**Key Disagreement**: [Main point of contention in 1 sentence]

Rules:
1. **Always respond in English**
2. Remain absolutely neutral
3. Be concise, no more than 120 words
"""

MODERATOR_FINAL_SUMMARY_EN = """You are a neutral, objective debate moderator.

**IMPORTANT: Respond in English only, even if the debate was in another language.**

Please generate a final summary report based on the complete debate (200-300 words):

Format:
## ğŸ“Š Debate Summary Report

### ğŸŸ¢ Optimist's Core Arguments
- [Argument 1]
- [Argument 2]

### ğŸ”´ Skeptic's Core Arguments
- [Argument 1]
- [Argument 2]

### âš–ï¸ Key Points of Disagreement
[What is the main disagreement between both sides? 1-2 sentences]

### ğŸ’¡ Overall Assessment
[Objectively analyze the strengths and weaknesses of both sides' arguments, 2-3 sentences]

### ğŸ¯ Conclusion & Recommendation
[Practical advice for the reader, 1 sentence]

Rules:
1. **Always respond in English**
2. Remain neutral and objective
3. Total word count 200-300 words
"""


# ============================================================
# å–å¾—å°æ‡‰èªè¨€çš„ Prompt
# ============================================================
def get_optimist_system() -> str:
    logger.info(f"ğŸŒ get_optimist_system: current_language={current_language}")
    return OPTIMIST_SYSTEM_EN if current_language == "en" else OPTIMIST_SYSTEM_ZH

def get_skeptic_system() -> str:
    logger.info(f"ğŸŒ get_skeptic_system: current_language={current_language}")
    return SKEPTIC_SYSTEM_EN if current_language == "en" else SKEPTIC_SYSTEM_ZH

def get_moderator_round_summary(round_num: int) -> str:
    template = MODERATOR_ROUND_SUMMARY_EN if current_language == "en" else MODERATOR_ROUND_SUMMARY_ZH
    return template.format(round=round_num)

def get_moderator_final_summary() -> str:
    return MODERATOR_FINAL_SUMMARY_EN if current_language == "en" else MODERATOR_FINAL_SUMMARY_ZH


# ============================================================
# å·¥å…·å®šç¾©ï¼ˆPhase 3b/3cï¼‰
# ============================================================

@tool
async def web_search_tool(query: str) -> str:
    """Search the web for latest information, statistics, or facts.

    Use this tool when you need:
    - Latest data or statistics
    - Specific event dates and details
    - Scientific research results
    - Market trends or business information

    Args:
        query: Search keywords (concise and clear)

    Returns:
        Formatted search results summary
    """
    from app.tools.search import web_search
    
    logger.debug(f"web_search_tool called with query: {query}")
    result = await web_search(query, language=current_language)
    
    # æ ¹æ“šèªè¨€è¿”å›ä¸åŒçš„éŒ¯èª¤è¨Šæ¯
    fallback = "Search failed" if current_language == "en" else "æœå°‹å¤±æ•—"
    return result.get("formatted", fallback)


# å·¥å…·åˆ—è¡¨ï¼ˆç”¨æ–¼ ToolNodeï¼‰
tools = [web_search_tool]


# ============================================================
# LLM å·¥å» ï¼ˆå« Rate Limit æ¨¡å‹åˆ‡æ›ï¼‰
# ============================================================

# é è¨­å‚™ç”¨æ¨¡å‹åˆ—è¡¨ï¼ˆæŒ‰å„ªå…ˆé †åºæ’åˆ—ï¼‰
DEFAULT_FALLBACK_MODELS = [
    "moonshotai/kimi-k2-instruct-0905",  # Moonshot Kimi K2ï¼Œé«˜å“è³ª
    "llama-3.1-8b-instant",               # é«˜é…é¡ï¼Œå¿«é€Ÿ
]


class RateLimitRetryLLM:
    """LLM åŒ…è£å™¨ï¼šé‡åˆ° 429 é™æµæ™‚è‡ªå‹•åˆ‡æ›æ¨¡å‹
    
    ç‰¹é»ï¼š
    - ç¶­è­·ä¸€å€‹ã€Œå·²é™æµã€æ¨¡å‹é›†åˆï¼ˆcooldown_modelsï¼‰
    - é‡åˆ° 429 æ™‚ï¼Œå°‡ç•¶å‰æ¨¡å‹åŠ å…¥ cooldownï¼Œåˆ‡æ›åˆ°ä¸‹ä¸€å€‹å¯ç”¨æ¨¡å‹é‡è©¦
    - æ‰€æœ‰æ¨¡å‹éƒ½é™æµæ™‚ï¼Œç­‰å¾… retry-after ç§’å¾Œé‡è©¦ä¸»æ¨¡å‹
    """
    
    # é¡ç´šåˆ¥è®Šæ•¸ï¼šè¿½è¹¤å…¨å±€é™æµç‹€æ…‹
    cooldown_models: set = set()
    
    def __init__(self, primary_model: str, fallback_models: list, bind_tools: bool = False):
        self.primary_model = primary_model
        self.fallback_models = fallback_models
        self.bind_tools = bind_tools
        self.api_key = os.getenv("GROQ_API_KEY")
        self._current_model = primary_model
        self._tools = tools if bind_tools else None
    
    def _get_available_model(self) -> str:
        """å–å¾—ç›®å‰å¯ç”¨çš„æ¨¡å‹ï¼ˆæ’é™¤å·²é™æµçš„ï¼‰"""
        all_models = [self.primary_model] + self.fallback_models
        for model in all_models:
            if model not in RateLimitRetryLLM.cooldown_models:
                return model
        # æ‰€æœ‰æ¨¡å‹éƒ½é™æµï¼Œè¿”å›ä¸»æ¨¡å‹ï¼ˆæœƒè§¸ç™¼ç­‰å¾…ï¼‰
        logger.warning("All models in cooldown, resetting and using primary model")
        RateLimitRetryLLM.cooldown_models.clear()
        return self.primary_model
    
    def _create_llm(self, model_name: str):
        """å»ºç«‹ ChatGroq å¯¦ä¾‹"""
        llm = ChatGroq(
            model=model_name,
            temperature=0.7,
            api_key=self.api_key,
            streaming=True
        )
        if self._tools:
            return llm.bind_tools(self._tools)
        return llm
    
    async def ainvoke(self, messages, **kwargs):
        """ç•°æ­¥èª¿ç”¨ LLMï¼Œé‡åˆ°é™æµè‡ªå‹•åˆ‡æ›æ¨¡å‹"""
        import asyncio
        
        max_retries = len(self.fallback_models) + 2  # å˜—è©¦æ‰€æœ‰æ¨¡å‹ + é¡å¤–é‡è©¦
        last_error = None
        
        for attempt in range(max_retries):
            current_model = self._get_available_model()
            llm = self._create_llm(current_model)
            
            try:
                logger.debug(f"RateLimitRetryLLM: attempt {attempt + 1}, model={current_model}")
                response = await llm.ainvoke(messages, **kwargs)
                
                # æˆåŠŸå¾Œï¼Œå¾ cooldown ç§»é™¤æ­¤æ¨¡å‹ï¼ˆå¦‚æœä¹‹å‰è¢«åŠ å…¥ï¼‰
                RateLimitRetryLLM.cooldown_models.discard(current_model)
                self._current_model = current_model
                return response
                
            except Exception as e:
                error_str = str(e).lower()
                
                # æª¢æŸ¥æ˜¯å¦ç‚º 429 é™æµéŒ¯èª¤
                if "429" in str(e) or "rate" in error_str or "too many" in error_str:
                    logger.warning(f"RateLimitRetryLLM: model {current_model} hit rate limit, switching...")
                    RateLimitRetryLLM.cooldown_models.add(current_model)
                    last_error = e
                    
                    # å¦‚æœé‚„æœ‰å…¶ä»–æ¨¡å‹å¯ç”¨ï¼Œç«‹å³é‡è©¦
                    if len(RateLimitRetryLLM.cooldown_models) < len(self.fallback_models) + 1:
                        continue
                    else:
                        # æ‰€æœ‰æ¨¡å‹éƒ½é™æµï¼Œç­‰å¾…ä¸€æ®µæ™‚é–“å¾Œé‡ç½®
                        logger.warning("All models rate limited, waiting 10 seconds...")
                        await asyncio.sleep(10)
                        RateLimitRetryLLM.cooldown_models.clear()
                        continue
                else:
                    # éé™æµéŒ¯èª¤ï¼Œç›´æ¥æ‹‹å‡º
                    raise
        
        # æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—
        raise last_error or Exception("All LLM retry attempts failed")
    
    @property
    def current_model(self) -> str:
        """è¿”å›ç•¶å‰ä½¿ç”¨çš„æ¨¡å‹"""
        return self._current_model


def get_llm(bind_tools: bool = False):
    """å–å¾— LLM å¯¦ä¾‹ï¼ˆå« Rate Limit å®¹éŒ¯ï¼‰

    Args:
        bind_tools: æ˜¯å¦ç¶å®šå·¥å…·ï¼ˆPhase 3cï¼‰
    
    ç’°å¢ƒè®Šæ•¸ï¼š
        GROQ_MODEL: ä¸»è¦æ¨¡å‹ï¼ˆé è¨­ llama-3.1-8b-instantï¼‰
        GROQ_FALLBACK_MODELS: å‚™ç”¨æ¨¡å‹ï¼Œé€—è™Ÿåˆ†éš”ï¼ˆå¯é¸ï¼‰
    """
    primary_model = os.getenv("GROQ_MODEL", "llama-3.1-8b-instant")
    
    # è§£æå‚™ç”¨æ¨¡å‹åˆ—è¡¨
    fallback_env = os.getenv("GROQ_FALLBACK_MODELS", "")
    if fallback_env:
        fallback_models = [m.strip() for m in fallback_env.split(",") if m.strip()]
    else:
        # ä½¿ç”¨é è¨­åˆ—è¡¨ï¼Œä½†æ’é™¤ä¸»æ¨¡å‹
        fallback_models = [m for m in DEFAULT_FALLBACK_MODELS if m != primary_model]
    
    logger.debug(f"get_llm: primary={primary_model}, fallbacks={fallback_models}, bind_tools={bind_tools}")
    
    return RateLimitRetryLLM(
        primary_model=primary_model,
        fallback_models=fallback_models,
        bind_tools=bind_tools
    )


# ============================================================
# è¼”åŠ©å‡½æ•¸
# ============================================================
def format_messages(messages: List[BaseMessage], limit: int = 4) -> str:
    """æ ¼å¼åŒ–æœ€è¿‘çš„è¨Šæ¯æ­·å²"""
    recent = messages[-limit:] if len(messages) > limit else messages
    lines = []
    for m in recent:
        name = getattr(m, 'name', None) or m.__class__.__name__
        if hasattr(m, 'content') and m.content:
            lines.append(f"[{name}]: {m.content}")
    
    if lines:
        return "\n".join(lines)
    else:
        return "(No conversation yet)" if current_language == "en" else "(å°šç„¡å°è©±)"


def build_prompt(state: DebateState, speaker: str) -> List[BaseMessage]:
    """ç‚ºæŒ‡å®šç™¼è¨€è€…å»ºæ§‹ promptï¼ˆåªç”¨æ–¼é¦–æ¬¡èª¿ç”¨ï¼‰"""
    history = format_messages(state['messages'])
    round_num = state['round_count'] + 1
    is_en = current_language == "en"
    
    # èªè¨€æŒ‡ç¤ºï¼ˆé–‹é ­å’Œçµå°¾ï¼‰
    lang_start = "*** RESPOND IN ENGLISH ONLY ***\n\n" if is_en else ""
    lang_end = "\n\n*** IMPORTANT: Your response MUST be in English! ***" if is_en else ""
    
    if speaker == "optimist":
        system = get_optimist_system()
        if state['round_count'] == 0 and len(state['messages']) == 0:
            if is_en:
                user_content = f"""{lang_start}Debate Topic: {state['topic']}

Opening statement, please speak as the Optimist.{lang_end}"""
            else:
                user_content = f"""è¾¯è«–ä¸»é¡Œï¼š{state['topic']}

é–‹å ´ç™½ï¼Œè«‹ä»¥æ¨‚è§€è€…èº«ä»½ç™¼è¨€ã€‚"""
        else:
            if is_en:
                user_content = f"""{lang_start}Debate Topic: {state['topic']}

Round {round_num}, please speak as the Optimist.

Conversation History:
{history}{lang_end}"""
            else:
                user_content = f"""è¾¯è«–ä¸»é¡Œï¼š{state['topic']}

ç¬¬ {round_num} è¼ªï¼Œè«‹ä»¥æ¨‚è§€è€…èº«ä»½ç™¼è¨€ã€‚

å°è©±æ­·å²ï¼š
{history}"""
    else:  # skeptic
        system = get_skeptic_system()
        if is_en:
            user_content = f"""{lang_start}Debate Topic: {state['topic']}

Round {round_num}, please refute the Optimist's arguments as the Skeptic.

Conversation History:
{history}{lang_end}"""
        else:
            user_content = f"""è¾¯è«–ä¸»é¡Œï¼š{state['topic']}

ç¬¬ {round_num} è¼ªï¼Œè«‹ä»¥æ‡·ç–‘è€…èº«ä»½åé§æ¨‚è§€è€…çš„è«–é»ã€‚

å°è©±æ­·å²ï¼š
{history}"""
    
    return [
        SystemMessage(content=system),
        HumanMessage(content=user_content)
    ]


def update_state_after_speaker(state: DebateState, speaker: str, content: str) -> DebateState:
    """ç™¼è¨€çµæŸå¾Œæ›´æ–°ç‹€æ…‹ï¼ˆä¿ç•™ä¾› real_debate_stream ä½¿ç”¨ï¼‰"""
    new_messages = state['messages'] + [AIMessage(content=content, name=speaker)]
    
    if speaker == "optimist":
        return {
            **state,
            "messages": new_messages,
            "current_speaker": "skeptic"
        }
    else:
        new_round = state['round_count'] + 1
        next_speaker = "end" if new_round >= state['max_rounds'] else "optimist"
        return {
            **state,
            "messages": new_messages,
            "current_speaker": next_speaker,
            "round_count": new_round
        }


def create_initial_state(topic: str, max_rounds: int = 3) -> DebateState:
    """å»ºç«‹åˆå§‹ç‹€æ…‹ï¼ˆPhase 3cï¼‰"""
    return {
        "messages": [],
        "topic": topic,
        "current_speaker": "optimist",
        "round_count": 0,
        "max_rounds": max_rounds,
        "tool_iterations": 0,
        "last_agent": ""
    }


# ============================================================
# LangGraph StateGraphï¼ˆPhase 3c - ToolNode æ¶æ§‹ï¼‰
# ============================================================

MAX_TOOL_ITERATIONS = 3


async def optimist_node(state: DebateState) -> dict:
    """æ¨‚è§€è€…ç¯€é»ï¼ˆPhase 3c: åƒ…æ±ºç­–ï¼Œä¸åŸ·è¡Œå·¥å…·ï¼‰
    
    è¿”å›çš„ AIMessage å¯èƒ½åŒ…å« tool_callsï¼Œç”±æ¢ä»¶é‚Šæ±ºå®šä¸‹ä¸€æ­¥
    """
    logger.debug("optimist_node: entering")
    
    # æª¢æŸ¥æ˜¯å¦å·²é”åˆ°å·¥å…·è¿­ä»£ä¸Šé™
    tool_iterations = state.get('tool_iterations', 0)
    should_bind_tools = tool_iterations < MAX_TOOL_ITERATIONS
    
    llm = get_llm(bind_tools=should_bind_tools)
    logger.debug(f"optimist_node: tool_iterations={tool_iterations}, bind_tools={should_bind_tools}")
    
    messages = state.get('messages', [])
    if messages and isinstance(messages[-1], ToolMessage):
        # å¾å·¥å…·è¿”å›ï¼šæå–å·¥å…·çµæœä½œç‚ºæ–‡å­—
        is_en = current_language == "en"
        tool_results = []
        for msg in reversed(messages[-6:]):
            if isinstance(msg, ToolMessage):
                prefix = "[Search Result]: " if is_en else "[æœå°‹çµæœ]: "
                tool_results.insert(0, f"{prefix}{msg.content}")
        
        tool_context = "\n".join(tool_results) if tool_results else ""
        history = format_messages(messages)
        
        if is_en:
            user_prompt = f"""*** RESPOND IN ENGLISH ONLY ***

Debate Topic: {state['topic']}

{tool_context}

Based on the search results above, please continue speaking as the Optimist. (Please respond directly without searching again)

Conversation History:
{history}

*** IMPORTANT: Your response MUST be in English! ***"""
        else:
            user_prompt = f"""è¾¯è«–ä¸»é¡Œï¼š{state['topic']}

{tool_context}

è«‹æ ¹æ“šä»¥ä¸Šæœå°‹çµæœï¼Œä»¥æ¨‚è§€è€…èº«ä»½ç¹¼çºŒç™¼è¨€ã€‚ï¼ˆè«‹ç›´æ¥ç™¼è¨€ï¼Œä¸è¦å†æœå°‹ï¼‰

å°è©±æ­·å²ï¼š
{history}"""
        
        prompt_messages = [
            SystemMessage(content=get_optimist_system()),
            HumanMessage(content=user_prompt)
        ]
    else:
        # é¦–æ¬¡èª¿ç”¨
        prompt_messages = build_prompt(state, "optimist")
    
    response = await llm.ainvoke(prompt_messages)
    logger.debug(f"optimist_node: response has tool_calls={bool(getattr(response, 'tool_calls', None))}")
    
    # æª¢æŸ¥æ˜¯å¦æœ‰å·¥å…·èª¿ç”¨
    has_tool_calls = hasattr(response, 'tool_calls') and response.tool_calls
    
    if has_tool_calls:
        # â„¹ï¸ Groq è¦æ±‚æ‰€æœ‰ AIMessage éƒ½æœ‰ name å±¬æ€§
        response_with_name = AIMessage(
            content=response.content or "",
            tool_calls=response.tool_calls,
            name="optimist"
        )
        return {
            "messages": [response_with_name],
            "current_speaker": "tools",
            "last_agent": "optimist"
        }
    else:
        # ç¢ºä¿ content æœ‰ name å±¬æ€§
        final_response = AIMessage(content=response.content or "(ç„¡å›æ‡‰)", name="optimist")
        return {
            "messages": [final_response],
            "current_speaker": "skeptic",
            "last_agent": "optimist",
            "tool_iterations": 0  # é‡ç½®
        }


async def skeptic_node(state: DebateState) -> dict:
    """æ‡·ç–‘è€…ç¯€é»ï¼ˆPhase 3c: åƒ…æ±ºç­–ï¼Œä¸åŸ·è¡Œå·¥å…·ï¼‰"""
    logger.debug("skeptic_node: entering")
    
    # æª¢æŸ¥æ˜¯å¦å·²é”åˆ°å·¥å…·è¿­ä»£ä¸Šé™
    tool_iterations = state.get('tool_iterations', 0)
    should_bind_tools = tool_iterations < MAX_TOOL_ITERATIONS
    
    llm = get_llm(bind_tools=should_bind_tools)
    logger.debug(f"skeptic_node: tool_iterations={tool_iterations}, bind_tools={should_bind_tools}")
    
    messages = state.get('messages', [])
    if messages and isinstance(messages[-1], ToolMessage):
        # å¾å·¥å…·è¿”å›ï¼šæå–å·¥å…·çµæœä½œç‚ºæ–‡å­—
        is_en = current_language == "en"
        tool_results = []
        for msg in reversed(messages[-6:]):
            if isinstance(msg, ToolMessage):
                prefix = "[Search Result]: " if is_en else "[æœå°‹çµæœ]: "
                tool_results.insert(0, f"{prefix}{msg.content}")
        
        tool_context = "\n".join(tool_results) if tool_results else ""
        history = format_messages(messages)
        
        if is_en:
            user_prompt = f"""*** RESPOND IN ENGLISH ONLY ***

Debate Topic: {state['topic']}

{tool_context}

Based on the search results above, please continue refuting as the Skeptic. (Please respond directly without searching again)

Conversation History:
{history}

*** IMPORTANT: Your response MUST be in English! ***"""
        else:
            user_prompt = f"""è¾¯è«–ä¸»é¡Œï¼š{state['topic']}

{tool_context}

è«‹æ ¹æ“šä»¥ä¸Šæœå°‹çµæœï¼Œä»¥æ‡·ç–‘è€…èº«ä»½ç¹¼çºŒåé§ã€‚ï¼ˆè«‹ç›´æ¥ç™¼è¨€ï¼Œä¸è¦å†æœå°‹ï¼‰

å°è©±æ­·å²ï¼š
{history}"""
        
        prompt_messages = [
            SystemMessage(content=get_skeptic_system()),
            HumanMessage(content=user_prompt)
        ]
    else:
        prompt_messages = build_prompt(state, "skeptic")
    
    response = await llm.ainvoke(prompt_messages)
    logger.debug(f"skeptic_node: response has tool_calls={bool(getattr(response, 'tool_calls', None))}")
    
    has_tool_calls = hasattr(response, 'tool_calls') and response.tool_calls
    
    if has_tool_calls:
        # â„¹ï¸ Groq è¦æ±‚æ‰€æœ‰ AIMessage éƒ½æœ‰ name å±¬æ€§
        response_with_name = AIMessage(
            content=response.content or "",
            tool_calls=response.tool_calls,
            name="skeptic"
        )
        return {
            "messages": [response_with_name],
            "current_speaker": "tools",
            "last_agent": "skeptic"
        }
    else:
        # Phase 3d: å°å‘ Moderatorï¼ˆè€Œé Optimist æˆ– ENDï¼‰
        final_response = AIMessage(content=response.content or "(ç„¡å›æ‡‰)", name="skeptic")
        
        # âš ï¸ ä¸å†åœ¨æ­¤å¢åŠ è¼ªæ•¸ï¼Œäº¤ç”± Moderator è™•ç†
        return {
            "messages": [final_response],
            "current_speaker": "moderator",
            "last_agent": "skeptic",
            "tool_iterations": 0
        }


async def tool_callback_node(state: DebateState) -> dict:
    """å·¥å…·åŸ·è¡Œå¾Œçš„å›èª¿ç¯€é»
    
    æ±ºå®šè¿”å›å“ªå€‹ Agentï¼Œä¸¦æª¢æŸ¥è¿­ä»£é™åˆ¶
    """
    logger.debug("tool_callback_node: entering")
    iterations = state.get("tool_iterations", 0) + 1
    last_agent = state.get("last_agent", "optimist")
    
    logger.debug(f"tool_callback_node: iterations={iterations}, last_agent={last_agent}")
    
    # æª¢æŸ¥æ˜¯å¦è¶…éé™åˆ¶
    if iterations >= MAX_TOOL_ITERATIONS:
        logger.warning(f"tool_callback_node: max iterations reached, forcing return to {last_agent}")
        return {
            "current_speaker": last_agent,
            "tool_iterations": 0
        }
    
    return {
        "current_speaker": last_agent,
        "tool_iterations": iterations
    }


def should_continue(state: DebateState) -> str:
    """è·¯ç”±å‡½æ•¸ï¼šæ ¹æ“š current_speaker æ±ºå®šä¸‹ä¸€å€‹ç¯€é»"""
    speaker = state.get("current_speaker", "end")
    logger.debug(f"should_continue: current_speaker={speaker}")
    return speaker


# ============================================================
# Phase 3d: Moderator ç¯€é»
# ============================================================

async def moderator_node(state: DebateState) -> dict:
    """ä¸»æŒäººç¯€é»ï¼šç”Ÿæˆéšæ®µæ€§æˆ–æœ€çµ‚ç¸½çµ

    é‚è¼¯ï¼š
    - è¼ªæ¬¡ < max_rounds: éšæ®µæ€§ç¸½çµ â†’ è¿”å› optimist
    - è¼ªæ¬¡ = max_rounds: æœ€çµ‚ç¸½çµ â†’ è¿”å› end
    """
    logger.debug("moderator_node: entering")

    llm = get_llm(bind_tools=False)  # ä¸ç¶å®šå·¥å…·

    current_round = state.get("round_count", 0) + 1  # Moderator åŸ·è¡Œæ™‚é‚„æœª ++
    max_rounds = state.get("max_rounds", 3)
    is_final = (current_round >= max_rounds)

    # é¸æ“‡ Prompt
    is_en = current_language == "en"
    
    if is_final:
        system_prompt = get_moderator_final_summary()
        if is_en:
            prompt_context = f"""*** RESPOND IN ENGLISH ONLY ***

Debate Topic: {state['topic']}

Complete Debate Record:
{format_messages(state['messages'], limit=30)}

Please generate the final summary report.

*** IMPORTANT: Your response MUST be in English! ***"""
        else:
            prompt_context = f"""è¾¯è«–ä¸»é¡Œï¼š{state['topic']}

å®Œæ•´è¾¯è«–è¨˜éŒ„ï¼š
{format_messages(state['messages'], limit=30)}

è«‹ç”Ÿæˆæœ€çµ‚ç¸½çµå ±å‘Šã€‚"""
    else:
        system_prompt = get_moderator_round_summary(current_round)

        # âš ï¸ åªæå–æœ¬è¼ªçš„ Optimist/Skeptic å°è©±ï¼ˆé¿å…åŒ…å«èˆŠçš„ Moderator ç¸½çµï¼‰
        recent_debate_msgs = [
            m for m in state['messages'][-8:]
            if getattr(m, 'name', None) in ("optimist", "skeptic")
        ]

        if is_en:
            prompt_context = f"""*** RESPOND IN ENGLISH ONLY ***

Debate Topic: {state['topic']}

This Round's Dialogue:
{format_messages(recent_debate_msgs, limit=6)}

Please generate the Round {current_round} summary.

*** IMPORTANT: Your response MUST be in English! ***"""
        else:
            prompt_context = f"""è¾¯è«–ä¸»é¡Œï¼š{state['topic']}

æœ¬è¼ªå°è©±ï¼š
{format_messages(recent_debate_msgs, limit=6)}

è«‹ç”Ÿæˆç¬¬ {current_round} è¼ªå°çµã€‚"""

    prompt_messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=prompt_context)
    ]

    response = await llm.ainvoke(prompt_messages)
    final_response = AIMessage(
        content=response.content or "(ç„¡æ³•ç”Ÿæˆç¸½çµ)",
        name="moderator"
    )

    # æ±ºå®šä¸‹ä¸€æ­¥
    new_round = current_round  # Moderator å®Œæˆå¾Œæ‰æ›´æ–°è¼ªæ•¸
    next_speaker = "end" if is_final else "optimist"

    logger.debug(f"moderator_node: round={new_round}, next={next_speaker}")

    return {
        "messages": [final_response],
        "current_speaker": next_speaker,
        "round_count": new_round,  # âš ï¸ é‡è¦ï¼šModerator è² è²¬æ›´æ–°è¼ªæ•¸
        "tool_iterations": 0  # æ¸…é›¶å·¥å…·è¨ˆæ•¸ï¼Œè®“ä¸‹ä¸€è¼ªå¯ä»¥ä½¿ç”¨å·¥å…·
    }


# ============================================================
# å»ºç«‹ StateGraphï¼ˆPhase 3dï¼‰
# ============================================================

# å»ºç«‹ ToolNode
tool_node = ToolNode(tools)

# å»ºç«‹ StateGraph
_graph = StateGraph(DebateState)

# æ·»åŠ ç¯€é»
_graph.add_node("optimist", optimist_node)
_graph.add_node("skeptic", skeptic_node)
_graph.add_node("tools", tool_node)
_graph.add_node("tool_callback", tool_callback_node)
_graph.add_node("moderator", moderator_node)  # Phase 3d: æ–°å¢

# è¨­å®šå…¥å£é»
_graph.set_conditional_entry_point(
    should_continue,
    {
        "optimist": "optimist",
        "skeptic": "skeptic",
        "tools": "tools",
        "moderator": "moderator",  # Phase 3d: æ–°å¢
        "end": END
    }
)

# Optimist å¾Œçš„è·¯ç”±
_graph.add_conditional_edges(
    "optimist",
    should_continue,
    {
        "tools": "tools",
        "skeptic": "skeptic",
        "tool_callback": "tool_callback",
        "moderator": "moderator",  # Phase 3d: å®¹éŒ¯è·¯ç”±
        "end": END
    }
)

# Skeptic å¾Œçš„è·¯ç”±
_graph.add_conditional_edges(
    "skeptic",
    should_continue,
    {
        "tools": "tools",
        "optimist": "optimist",
        "tool_callback": "tool_callback",
        "moderator": "moderator",  # Phase 3d: ä¸»è¦è·¯ç”±
        "end": END
    }
)

# Tool åŸ·è¡Œå¾Œé€²å…¥å›èª¿
_graph.add_edge("tools", "tool_callback")

# Tool å›èª¿å¾Œè¿”å› Agent
_graph.add_conditional_edges(
    "tool_callback",
    should_continue,
    {
        "optimist": "optimist",
        "skeptic": "skeptic",
        "moderator": "moderator",  # Phase 3d: å®¹éŒ¯è·¯ç”±
        "end": END
    }
)

# Moderator å¾Œçš„è·¯ç”±ï¼ˆPhase 3dï¼‰
_graph.add_conditional_edges(
    "moderator",
    should_continue,
    {
        "optimist": "optimist",  # æœªæ»¿ 3 è¼ªï¼Œç¹¼çºŒè¾¯è«–
        "end": END               # å·²æ»¿ 3 è¼ªï¼ŒçµæŸ
    }
)

# ç·¨è­¯ç‚ºå¯åŸ·è¡Œçš„ graph
debate_graph = _graph.compile()

logger.info("debate_graph compiled successfully (Phase 3d: Moderator Agent)")

